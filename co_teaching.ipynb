{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torch in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (2.2.2)\n",
      "Requirement already satisfied: filelock in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from torch) (3.16.1)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from torch) (4.12.2)\n",
      "Requirement already satisfied: sympy in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from torch) (1.13.3)\n",
      "Requirement already satisfied: networkx in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from torch) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from torch) (3.1.4)\n",
      "Requirement already satisfied: fsspec in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from torch) (2024.10.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from jinja2->torch) (3.0.2)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from sympy->torch) (1.3.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: transformers in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (4.46.3)\n",
      "Requirement already satisfied: filelock in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from transformers) (3.16.1)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.23.2 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from transformers) (0.26.2)\n",
      "Requirement already satisfied: numpy>=1.17 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from transformers) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from transformers) (21.3)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from transformers) (6.0.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from transformers) (2024.11.6)\n",
      "Requirement already satisfied: requests in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from transformers) (2.32.3)\n",
      "Requirement already satisfied: tokenizers<0.21,>=0.20 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from transformers) (0.20.3)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from transformers) (0.4.5)\n",
      "Requirement already satisfied: tqdm>=4.27 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from transformers) (4.66.5)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.23.2->transformers) (2024.10.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.23.2->transformers) (4.12.2)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from packaging>=20.0->transformers) (3.2.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from requests->transformers) (3.4.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from requests->transformers) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from requests->transformers) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from requests->transformers) (2024.8.30)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: wandb in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (0.18.7)\n",
      "Requirement already satisfied: click!=8.0.0,>=7.1 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from wandb) (8.1.7)\n",
      "Requirement already satisfied: docker-pycreds>=0.4.0 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from wandb) (0.4.0)\n",
      "Requirement already satisfied: gitpython!=3.1.29,>=1.0.0 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from wandb) (3.1.43)\n",
      "Requirement already satisfied: platformdirs in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from wandb) (4.3.6)\n",
      "Requirement already satisfied: protobuf!=4.21.0,!=5.28.0,<6,>=3.19.0 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from wandb) (4.25.5)\n",
      "Requirement already satisfied: psutil>=5.0.0 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from wandb) (6.0.0)\n",
      "Requirement already satisfied: pyyaml in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from wandb) (6.0.2)\n",
      "Requirement already satisfied: requests<3,>=2.0.0 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from wandb) (2.32.3)\n",
      "Requirement already satisfied: sentry-sdk>=2.0.0 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from wandb) (2.18.0)\n",
      "Requirement already satisfied: setproctitle in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from wandb) (1.3.4)\n",
      "Requirement already satisfied: setuptools in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from wandb) (75.2.0)\n",
      "Requirement already satisfied: typing-extensions<5,>=4.4 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from wandb) (4.12.2)\n",
      "Requirement already satisfied: six>=1.4.0 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from docker-pycreds>=0.4.0->wandb) (1.16.0)\n",
      "Requirement already satisfied: gitdb<5,>=4.0.1 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from gitpython!=3.1.29,>=1.0.0->wandb) (4.0.11)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from requests<3,>=2.0.0->wandb) (3.4.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from requests<3,>=2.0.0->wandb) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from requests<3,>=2.0.0->wandb) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from requests<3,>=2.0.0->wandb) (2024.8.30)\n",
      "Requirement already satisfied: smmap<6,>=3.0.1 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from gitdb<5,>=4.0.1->gitpython!=3.1.29,>=1.0.0->wandb) (5.0.1)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install torch\n",
    "%pip install transformers\n",
    "%pip install wandb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cache Space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: LC_ALL=C.UTF-8\n",
      "env: LANG=C.UTF-8\n",
      "env: TRANSFORMERS_CACHE=/home/ec2-user/SageMaker/cache\n",
      "env: HF_DATASETS_CACHE=/home/ec2-user/SageMaker/cache\n",
      "env: CUDA_LAUNCH_BLOCKING=1\n"
     ]
    }
   ],
   "source": [
    "%env LC_ALL=C.UTF-8\n",
    "%env LANG=C.UTF-8\n",
    "%env TRANSFORMERS_CACHE=/home/ec2-user/SageMaker/cache\n",
    "%env HF_DATASETS_CACHE=/home/ec2-user/SageMaker/cache\n",
    "%env CUDA_LAUNCH_BLOCKING=1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import Lib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages/transformers/utils/hub.py:128: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import yaml\n",
    "import random, os\n",
    "import argparse\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from tqdm import tqdm\n",
    "from torch.autograd import Variable\n",
    "from tqdm import tqdm\n",
    "import transformers\n",
    "from torch.nn import NLLLoss\n",
    "import logging\n",
    "import json\n",
    "import torch.nn as nn\n",
    "import re\n",
    "from torch.autograd import Variable\n",
    "import wandb\n",
    "from pathlib import Path\n",
    "import math\n",
    "\n",
    "\n",
    "logger = logging.getLogger(__name__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def reform_label(tokens, label, tokenizer, max_seq_length):\n",
    "    new_tokens = list()\n",
    "    new_label = list()\n",
    "    for step, token in enumerate(tokens[:-1]):\n",
    "        split_token = tokenizer.tokenize(token)\n",
    "        if len(split_token) > 0:\n",
    "            new_tokens.extend(split_token)\n",
    "            \n",
    "    new_label.append(label)\n",
    "    new_tokens = new_tokens[:max_seq_length] + ['EOS']\n",
    "    \n",
    "    return new_tokens, new_label\n",
    "\n",
    "def tok2int_sent(example, tokenizer, max_seq_length):\n",
    "    src_tokens = example[0]\n",
    "    src_label = example[1]\n",
    "\n",
    "    src_tokens, src_label = reform_label(src_tokens, src_label, tokenizer, max_seq_length)\n",
    "\n",
    "    tokens = src_tokens\n",
    "    tokens = [\"[CLS]\"] + tokens\n",
    "    label = src_label\n",
    "    input_seg = [1] * len(tokens)\n",
    "\n",
    "    input_ids = tokenizer.convert_tokens_to_ids(tokens)\n",
    "    input_mask = [1] * len(input_ids)\n",
    "\n",
    "    max_len = max_seq_length * 2 + 3\n",
    "    padding = [0] * (max_len - len(input_ids))\n",
    "    input_ids += padding\n",
    "    input_mask += padding\n",
    "    input_seg += padding\n",
    "\n",
    "    return input_ids, input_mask, input_seg, label\n",
    "\n",
    "def tok2int_list(data, tokenizer, max_seq_length):\n",
    "    inps = list()\n",
    "    msks = list()\n",
    "    segs = list()\n",
    "    labs = list()\n",
    "    for examples in data:\n",
    "        input_ids, input_mask, input_seg, labels = tok2int_sent(examples, tokenizer, max_seq_length)\n",
    "        inps.append(input_ids)\n",
    "        msks.append(input_mask)\n",
    "        segs.append(input_seg)\n",
    "        labs.append(labels)\n",
    "        \n",
    "    return inps, msks, segs, labs\n",
    "\n",
    "\n",
    "class DataLoader(object):\n",
    "    ''' For data iteration '''\n",
    "\n",
    "    def __init__(self, data_path, tokenizer, args, test=False, batch_size=64):\n",
    "        self.cuda = args.cuda\n",
    "        self.batch_size = batch_size\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_len = args.max_len\n",
    "        self.data_path = data_path\n",
    "        self.test = test\n",
    "        self.src_flag = args.src_flag\n",
    "        self.hyp_flag = args.hyp_flag\n",
    "        examples = self.read_file(data_path)\n",
    "        self.examples = examples\n",
    "        self.total_num = len(examples)\n",
    "        if self.test:\n",
    "            self.total_step = np.ceil(self.total_num * 1.0 / batch_size)\n",
    "        else:\n",
    "            self.total_step = self.total_num / batch_size\n",
    "            self.shuffle()\n",
    "        self.step = 0\n",
    "    \n",
    "    def read_file(self, data_path):\n",
    "        data_list = list()\n",
    "        data = pd.read_csv(data_path)\n",
    "        data = data[data['grammarScore']>0]\n",
    "        data.dropna(inplace=True)\n",
    "\n",
    "        for _ , row in data.iterrows():\n",
    "            example = list()\n",
    "            line = row['transcript']\n",
    "            src_token = line.split()\n",
    "            src_label = float(row['grammarScore'])\n",
    "            example.append(src_token)\n",
    "            example.append(src_label)\n",
    "            data_list.append(example)\n",
    "        return data_list\n",
    "\n",
    "\n",
    "    def shuffle(self):\n",
    "        np.random.shuffle(self.examples)\n",
    "\n",
    "    def __iter__(self):\n",
    "        return self\n",
    "\n",
    "    def __next__(self):\n",
    "        return self.next()\n",
    "\n",
    "    def __len__(self):\n",
    "        return self._n_batch\n",
    "\n",
    "    def next(self):\n",
    "        ''' Get the next batch '''\n",
    "        if self.step < self.total_step:\n",
    "            examples = self.examples[self.step * self.batch_size : (self.step+1)*self.batch_size]\n",
    "            \n",
    "            inp, msk, seg, score = tok2int_list(examples, self.tokenizer, self.max_len)\n",
    "\n",
    "            inp_tensor = Variable(\n",
    "                torch.LongTensor(inp))\n",
    "            msk_tensor = Variable(\n",
    "                torch.LongTensor(msk))\n",
    "            seg_tensor = Variable(\n",
    "                torch.LongTensor(seg))\n",
    "            score_tensor = Variable(\n",
    "                torch.LongTensor(score))\n",
    "\n",
    "            if self.cuda:\n",
    "                inp_tensor = inp_tensor.cuda()\n",
    "                msk_tensor = msk_tensor.cuda()\n",
    "                seg_tensor = seg_tensor.cuda()\n",
    "                score_tensor = score_tensor.cuda()\n",
    "\n",
    "            self.step += 1\n",
    "            return inp_tensor, msk_tensor, seg_tensor, score_tensor\n",
    "\n",
    "        else:\n",
    "            self.step = 0\n",
    "            if not self.test:\n",
    "                self.shuffle()\n",
    "            raise StopIteration()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from scipy.stats import pearsonr,spearmanr\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import numpy as np\n",
    "\n",
    "def eval_result(predicts, labels):\n",
    "    spearman_corr, _ = spearmanr(predicts, labels)\n",
    "    corr, _ = pearsonr(predicts, labels)\n",
    "    res = {\"Prearson Corr\":corr,\"Spearman Corr\":spearman_corr,\"Eval Loss\":np.sqrt(mean_squared_error(predicts, labels))}\n",
    "    return res\n",
    "\n",
    "def eval_model(model, validset_reader):\n",
    "    model.eval()\n",
    "    predicts = list()\n",
    "    labels = list()\n",
    "    with torch.no_grad():\n",
    "        for step, (inp_tensor, msk_tensor, seg_tensor, score_tensor) in tqdm(enumerate(validset_reader)):\n",
    "            # print(\"Eval Input --\",inp_tensor)\n",
    "            # print(\"Eval msk_tensor --\",msk_tensor)\n",
    "            # print(\"Eval seg_tensor --\",seg_tensor)\n",
    "            \n",
    "            score_tensor = score_tensor.to(torch.float)\n",
    "            prob = model(inp_tensor, msk_tensor, seg_tensor)\n",
    "\n",
    "            # print(\"Eval output---->\",prob.view(-1).tolist())\n",
    "            \n",
    "            predict = prob.type_as(score_tensor).view(-1).tolist()\n",
    "            score = score_tensor.view(-1).tolist()\n",
    "            predicts.extend(predict)\n",
    "            labels.extend(score)\n",
    "            \n",
    "        results = eval_result(predicts, labels)\n",
    "            \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# list_net_loss\n",
    "\n",
    "def listnet_loss(y_i, z_i):\n",
    "    P_y_i = F.softmax(y_i.float(), dim=0)\n",
    "    P_z_i = F.softmax(z_i.float(), dim=0)\n",
    "    return - torch.sum(P_y_i * torch.log(P_z_i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Co teaching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def train_model_co_teaching(model1, model2, args, trainset_reader, validset_reader):\n",
    "    saved_checkpoints1 = []\n",
    "    saved_checkpoints2 = []\n",
    "    \n",
    "    save_path = args.outdir\n",
    "    forget_rate = args.forget_rate \n",
    "    num_gradual = int(args.num_train_epochs / 2)\n",
    "    running_loss1 = 0.0\n",
    "    running_loss2 = 0.0\n",
    "    running_loss  = 0.0\n",
    "\n",
    "    t_total = int(\n",
    "        trainset_reader.total_step / args.gradient_accumulation_steps * args.num_train_epochs\n",
    "    )\n",
    "\n",
    "    optimizer1 = transformers.AdamW(model1.parameters(), lr=args.learning_rate, eps=1e-8)\n",
    "    optimizer2 = transformers.AdamW(model2.parameters(), lr=args.learning_rate, eps=1e-8)\n",
    "\n",
    "    scheduler1 = transformers.get_linear_schedule_with_warmup(\n",
    "        optimizer1, num_warmup_steps=0, num_training_steps=t_total\n",
    "    )\n",
    "    scheduler2 = transformers.get_linear_schedule_with_warmup(\n",
    "        optimizer2, num_warmup_steps=0, num_training_steps=t_total\n",
    "    )\n",
    "\n",
    "    global_step = 0\n",
    "    for epoch in range(int(args.num_train_epochs)):\n",
    "        model1.train()\n",
    "        model2.train()\n",
    "\n",
    "        # current_forget_rate = min(forget_rate, forget_rate * (1 - epoch / num_gradual))\n",
    "        current_forget_rate = forget_rate\n",
    "\n",
    "        for inp_tensor, msk_tensor, seg_tensor, score_tensor in tqdm(trainset_reader):\n",
    "            optimizer1.zero_grad()\n",
    "            optimizer2.zero_grad()\n",
    "\n",
    "            pred1 = model1(inp_tensor, msk_tensor, seg_tensor).view(-1)\n",
    "            pred2 = model2(inp_tensor, msk_tensor, seg_tensor).view(-1)\n",
    "\n",
    "            score_tensor = score_tensor.view(-1).to(torch.float)\n",
    "\n",
    "            loss1 = F.mse_loss(pred1, score_tensor, reduction='none')\n",
    "            loss2 = F.mse_loss(pred2, score_tensor, reduction='none')\n",
    "\n",
    "            idx1_sorted = torch.argsort(loss1).detach()\n",
    "            idx2_sorted = torch.argsort(loss2).detach()\n",
    "\n",
    "            num_remember = int((1 - current_forget_rate) * len(loss1))\n",
    "\n",
    "            idx1 = idx1_sorted[:num_remember]\n",
    "            idx2 = idx2_sorted[:num_remember]\n",
    "\n",
    "            clean_loss1 = loss1[idx2].mean()\n",
    "            clean_loss2 = loss2[idx1].mean()\n",
    "            \n",
    "            clean_loss1.backward()\n",
    "            clean_loss2.backward()\n",
    "\n",
    "            running_loss1 += clean_loss1.item()\n",
    "            running_loss2 += clean_loss2.item()\n",
    "            running_loss += (clean_loss1.item()+clean_loss2.item())\n",
    "\n",
    "            global_step += 1\n",
    "            if global_step % args.gradient_accumulation_steps == 0:\n",
    "                optimizer1.step()\n",
    "                optimizer2.step()\n",
    "                scheduler1.step()\n",
    "                scheduler2.step()\n",
    "                optimizer1.zero_grad()\n",
    "                optimizer2.zero_grad()\n",
    "\n",
    "        logger.info(\n",
    "            f'Epoch: {epoch},Model Loss:{running_loss/global_step}, Loss Model1: {running_loss1 / global_step}, Loss Model2: {running_loss2 / global_step}, Forget Rate: {current_forget_rate}'\n",
    "        )\n",
    "        \n",
    "        train_res = {\n",
    "            \"Train Loss\": running_loss / global_step,\n",
    "            \"Train Loss 1\": running_loss1 / global_step,\n",
    "            \"Train Loss 2\": running_loss2 / global_step,\n",
    "            \"Learning Rate Model 1\": scheduler1.get_last_lr()[0],\n",
    "            \"Learning Rate Model 2\": scheduler2.get_last_lr()[0],\n",
    "        }\n",
    "\n",
    "        logger.info('Start eval for Model 1!')\n",
    "        result_dict1 = eval_model(model1, validset_reader)\n",
    "        logger.info(result_dict1)\n",
    "\n",
    "        logger.info('Start eval for Model 2!')\n",
    "        result_dict2 = eval_model(model2, validset_reader)\n",
    "        logger.info(result_dict2)\n",
    "\n",
    "        train_res.update({\"Model 1 Validation\": result_dict1, \"Model 2 Validation\": result_dict2})\n",
    "        wandb.log(train_res)\n",
    "\n",
    "        check_point_path1 = save_path + f\"/model1_{epoch}_best.pt\"\n",
    "        check_point_path2 = save_path + f\"/model2_{epoch}_best.pt\"\n",
    "\n",
    "        torch.save({'epoch': epoch,\n",
    "                    'model': model1.state_dict()},check_point_path1)\n",
    "        torch.save({'epoch': epoch,\n",
    "            'model': model2.state_dict()},check_point_path2)\n",
    "        \n",
    "        saved_checkpoints1.append(check_point_path1)\n",
    "        saved_checkpoints2.append(check_point_path2)\n",
    "\n",
    "        if len(saved_checkpoints1) > args.max_model_save:\n",
    "            old_checkpoint1 = saved_checkpoints1.pop(0)\n",
    "            old_checkpoint2 = saved_checkpoints2.pop(0)\n",
    "            \n",
    "            if os.path.exists(old_checkpoint1):\n",
    "                os.remove(old_checkpoint1)\n",
    "                \n",
    "            if os.path.exists(old_checkpoint2):\n",
    "                os.remove(old_checkpoint2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Co Teaching Plus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def train_model_co_teaching_plus(model1, model2, args, trainset_reader, validset_reader):\n",
    "    saved_checkpoints1 = []\n",
    "    saved_checkpoints2 = []\n",
    "    \n",
    "    save_path = args.outdir\n",
    "    best_acc = 0.0\n",
    "    running_loss = 0.0\n",
    "    running_loss1 = 0.0\n",
    "    running_loss2 = 0.0\n",
    "\n",
    "    forget_rate = args.forget_rate\n",
    "    num_gradual = int(args.num_train_epochs / 2)\n",
    "    rate_schedule = np.linspace(forget_rate, 0, num_gradual).tolist()\n",
    "\n",
    "    t_total = int(trainset_reader.total_step / args.gradient_accumulation_steps * args.num_train_epochs)\n",
    "\n",
    "    optimizer1 = transformers.AdamW(model1.parameters(), lr=args.learning_rate, eps=1e-8)\n",
    "    optimizer2 = transformers.AdamW(model2.parameters(), lr=args.learning_rate, eps=1e-8)\n",
    "\n",
    "    scheduler1 = transformers.get_linear_schedule_with_warmup(\n",
    "        optimizer1, num_warmup_steps=0, num_training_steps=t_total\n",
    "    )\n",
    "    scheduler2 = transformers.get_linear_schedule_with_warmup(\n",
    "        optimizer2, num_warmup_steps=0, num_training_steps=t_total\n",
    "    )\n",
    "\n",
    "    global_step = 0\n",
    "    for epoch in range(int(args.num_train_epochs)):\n",
    "        optimizer1.zero_grad()\n",
    "        optimizer2.zero_grad()\n",
    "\n",
    "        current_forget_rate = rate_schedule[epoch] if epoch < num_gradual else 0.0\n",
    "\n",
    "        for inp_tensor, msk_tensor, seg_tensor, score_tensor in tqdm(trainset_reader):\n",
    "            model1.train()\n",
    "            model2.train()\n",
    "\n",
    "            pred_score1 = model1(inp_tensor, msk_tensor, seg_tensor).view(-1)\n",
    "            pred_score2 = model2(inp_tensor, msk_tensor, seg_tensor).view(-1)\n",
    "\n",
    "            score_tensor = score_tensor.view(-1).to(torch.float)\n",
    "            \n",
    "            ##** Loss Based Selection **##\n",
    "\n",
    "#             loss1 = F.mse_loss(pred_score1, score_tensor, reduction='none')\n",
    "#             loss2 = F.mse_loss(pred_score2, score_tensor, reduction='none')\n",
    "\n",
    "#             idx1_sorted = torch.argsort(loss1).detach()\n",
    "#             idx2_sorted = torch.argsort(loss2).detach()\n",
    "\n",
    "#             num_remember = int((1 - current_forget_rate) * len(loss1))\n",
    "\n",
    "#             idx1_small = idx1_sorted[:num_remember]\n",
    "#             idx2_small = idx2_sorted[:num_remember]\n",
    "\n",
    "#             loss1_final = F.mse_loss(pred_score1[idx2_small], score_tensor[idx2_small])\n",
    "#             loss2_final = F.mse_loss(pred_score2[idx1_small], score_tensor[idx1_small])\n",
    "\n",
    "#             loss1_final.backward()\n",
    "#             loss2_final.backward()    \n",
    "\n",
    "            ##** Agreement Based Selection **##\n",
    "    \n",
    "            loss1 = F.mse_loss(pred_score1, score_tensor, reduction='none')\n",
    "            loss2 = F.mse_loss(pred_score2, score_tensor, reduction='none')\n",
    "            \n",
    "            pred_diff = torch.abs(pred_score1 - pred_score2)\n",
    "            agreement_mask = pred_diff <= args.agreement_threshold\n",
    "\n",
    "            loss1_agreed = loss1[agreement_mask]\n",
    "            loss2_agreed = loss2[agreement_mask]\n",
    "\n",
    "            idx1_sorted = torch.argsort(loss1_agreed).detach()\n",
    "            idx2_sorted = torch.argsort(loss2_agreed).detach()\n",
    "\n",
    "            num_remember = int((1 - current_forget_rate) * len(loss1_agreed))\n",
    "\n",
    "            idx1_small = idx1_sorted[:num_remember]\n",
    "            idx2_small = idx2_sorted[:num_remember]\n",
    "\n",
    "            loss1_final = F.mse_loss(pred_score1[idx2_small], score_tensor[idx2_small])\n",
    "            loss2_final = F.mse_loss(pred_score2[idx1_small], score_tensor[idx1_small])\n",
    "\n",
    "            loss1_final.backward()\n",
    "            loss2_final.backward()\n",
    "    \n",
    "            running_loss1 += loss1_final.item()\n",
    "            running_loss1 += loss2_final.item()\n",
    "            running_loss += (loss1_final.item() + loss2_final.item())\n",
    "            \n",
    "            global_step += 1\n",
    "            if global_step % args.gradient_accumulation_steps == 0:\n",
    "                optimizer1.step()\n",
    "                optimizer2.step()\n",
    "                scheduler1.step()\n",
    "                scheduler2.step()\n",
    "                optimizer1.zero_grad()\n",
    "                optimizer2.zero_grad()\n",
    "\n",
    "        logger.info('Epoch: {}, Loss:{}, Loss1:{}, Loss2: {}, LR1: {}, LR2: {}'.format(epoch, running_loss / global_step,running_loss1 / global_step, running_loss2 / global_step, scheduler1.get_last_lr()[0], scheduler2.get_last_lr()[0]))\n",
    "\n",
    "        train_res = {\n",
    "            \"Train Loss\": running_loss / global_step,\n",
    "            \"Train Loss 1\": running_loss1 / global_step,\n",
    "            \"Train Loss 2\": running_loss2 / global_step,\n",
    "            \"Learning Rate Model 1\": scheduler1.get_last_lr()[0],\n",
    "            \"Learning Rate Model 2\": scheduler2.get_last_lr()[0],\n",
    "        }\n",
    "\n",
    "        wandb.log(train_res)\n",
    "\n",
    "        logger.info('Start eval for Model 1!')\n",
    "        result_dict1 = eval_model(model1, validset_reader)\n",
    "        logger.info(result_dict1)\n",
    "\n",
    "        logger.info('Start eval for Model 2!')\n",
    "        result_dict2 = eval_model(model2, validset_reader)\n",
    "        logger.info(result_dict2)\n",
    "\n",
    "        train_res.update({\"Model 1 Validation\": result_dict1, \"Model 2 Validation\": result_dict2})\n",
    "        wandb.log(train_res)\n",
    "\n",
    "        check_point_path1 = save_path + f\"/model1_{epoch}_best.pt\"\n",
    "        check_point_path2 = save_path + f\"/model2_{epoch}_best.pt\"\n",
    "\n",
    "        torch.save({'epoch': epoch,\n",
    "                    'model': model1.state_dict()},check_point_path1)\n",
    "        torch.save({'epoch': epoch,\n",
    "            'model': model2.state_dict()},check_point_path2)\n",
    "        \n",
    "        saved_checkpoints1.append(check_point_path1)\n",
    "        saved_checkpoints2.append(check_point_path2)\n",
    "\n",
    "        if len(saved_checkpoints1) > args.max_model_save:\n",
    "            old_checkpoint1 = saved_checkpoints1.pop(0)\n",
    "            old_checkpoint2 = saved_checkpoints2.pop(0)\n",
    "            \n",
    "            if os.path.exists(old_checkpoint1):\n",
    "                os.remove(old_checkpoint1)\n",
    "                \n",
    "            if os.path.exists(old_checkpoint2):\n",
    "                os.remove(old_checkpoint2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Attention Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class inference_model(nn.Module):\n",
    "    def __init__(self, bert_model, args):\n",
    "        super(inference_model, self).__init__()\n",
    "        self.bert_hidden_dim = args.bert_hidden_dim\n",
    "        self.pred_model = bert_model\n",
    "        self.model_name = args.bert_pretrain\n",
    "        self.max_len = args.max_len * 2 + 3\n",
    "        self.proj_hidden = nn.Linear(self.bert_hidden_dim, 1)\n",
    "\n",
    "\n",
    "    def forward(self, inp_tensor, msk_tensor, seg_tensor,score_flag=True):\n",
    "        inp_tensor = inp_tensor.view(-1, self.max_len)\n",
    "        msk_tensor = msk_tensor.view(-1, self.max_len)\n",
    "        seg_tensor = seg_tensor.view(-1, self.max_len)\n",
    "        \n",
    "        if \"bert\" in self.model_name.lower():\n",
    "            outputs = self.pred_model(inp_tensor, msk_tensor, seg_tensor)            \n",
    "        elif \"electra\" in self.model_name.lower():\n",
    "            outputs = self.pred_model(inp_tensor, msk_tensor)\n",
    "        else:\n",
    "            BaseException (\"Not implement!\")\n",
    "        \n",
    "        pred_score = self.proj_hidden(outputs.pooler_output)\n",
    "\n",
    "        return pred_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[27-11-2024 02:30:06] INFO: <__main__.Config object at 0x7f44c02a7940>\n",
      "[27-11-2024 02:30:06] DEBUG: Starting new HTTPS connection (1): huggingface.co:443\n",
      "[27-11-2024 02:30:07] DEBUG: https://huggingface.co:443 \"HEAD /bert-base-uncased/resolve/main/tokenizer_config.json HTTP/11\" 200 0\n",
      "[27-11-2024 02:30:07] INFO: Start training!\n",
      "[27-11-2024 02:30:07] INFO: loading training set\n",
      "[27-11-2024 02:30:08] INFO: loading validation set\n",
      "[27-11-2024 02:30:08] INFO: initializing estimator model\n"
     ]
    }
   ],
   "source": [
    "class Config:\n",
    "    def __init__(self, **entries):\n",
    "        self.__dict__.update(entries)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    config_file = 'config.yaml'\n",
    "    with open(config_file, \"r\") as ymlfile:\n",
    "        config_dict = yaml.load(ymlfile, Loader=yaml.FullLoader)\n",
    "\n",
    "    args = Config(**config_dict)\n",
    "\n",
    "    if not os.path.exists(args.outdir):\n",
    "        Path(args.outdir).mkdir(parents=True, exist_ok=True)\n",
    "        \n",
    "    handlers = [logging.FileHandler(os.path.abspath(args.outdir) + '/train_log.txt'), logging.StreamHandler()]\n",
    "    logging.basicConfig(format='[%(asctime)s] %(levelname)s: %(message)s', level=logging.DEBUG,\n",
    "                        datefmt='%d-%m-%Y %H:%M:%S', handlers=handlers)\n",
    "    logger.info(args)\n",
    "\n",
    "    tokenizer = transformers.BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "    \n",
    "    logger.info('Start training!')\n",
    "    logger.info(\"loading training set\")\n",
    "    trainset_reader = DataLoader(args.train_path, tokenizer, args, batch_size=args.train_batch_size)\n",
    "    logger.info(\"loading validation set\")\n",
    "    validset_reader = DataLoader(args.test_path, tokenizer, args, batch_size=args.valid_batch_size)\n",
    "    logger.info('initializing estimator model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertModel were not initialized from the model checkpoint at /home/ec2-user/SageMaker/bert_model/model_best.pt and are newly initialized: ['embeddings.LayerNorm.bias', 'embeddings.LayerNorm.weight', 'embeddings.position_embeddings.weight', 'embeddings.token_type_embeddings.weight', 'embeddings.word_embeddings.weight', 'encoder.layer.0.attention.output.LayerNorm.bias', 'encoder.layer.0.attention.output.LayerNorm.weight', 'encoder.layer.0.attention.output.dense.bias', 'encoder.layer.0.attention.output.dense.weight', 'encoder.layer.0.attention.self.key.bias', 'encoder.layer.0.attention.self.key.weight', 'encoder.layer.0.attention.self.query.bias', 'encoder.layer.0.attention.self.query.weight', 'encoder.layer.0.attention.self.value.bias', 'encoder.layer.0.attention.self.value.weight', 'encoder.layer.0.intermediate.dense.bias', 'encoder.layer.0.intermediate.dense.weight', 'encoder.layer.0.output.LayerNorm.bias', 'encoder.layer.0.output.LayerNorm.weight', 'encoder.layer.0.output.dense.bias', 'encoder.layer.0.output.dense.weight', 'encoder.layer.1.attention.output.LayerNorm.bias', 'encoder.layer.1.attention.output.LayerNorm.weight', 'encoder.layer.1.attention.output.dense.bias', 'encoder.layer.1.attention.output.dense.weight', 'encoder.layer.1.attention.self.key.bias', 'encoder.layer.1.attention.self.key.weight', 'encoder.layer.1.attention.self.query.bias', 'encoder.layer.1.attention.self.query.weight', 'encoder.layer.1.attention.self.value.bias', 'encoder.layer.1.attention.self.value.weight', 'encoder.layer.1.intermediate.dense.bias', 'encoder.layer.1.intermediate.dense.weight', 'encoder.layer.1.output.LayerNorm.bias', 'encoder.layer.1.output.LayerNorm.weight', 'encoder.layer.1.output.dense.bias', 'encoder.layer.1.output.dense.weight', 'encoder.layer.10.attention.output.LayerNorm.bias', 'encoder.layer.10.attention.output.LayerNorm.weight', 'encoder.layer.10.attention.output.dense.bias', 'encoder.layer.10.attention.output.dense.weight', 'encoder.layer.10.attention.self.key.bias', 'encoder.layer.10.attention.self.key.weight', 'encoder.layer.10.attention.self.query.bias', 'encoder.layer.10.attention.self.query.weight', 'encoder.layer.10.attention.self.value.bias', 'encoder.layer.10.attention.self.value.weight', 'encoder.layer.10.intermediate.dense.bias', 'encoder.layer.10.intermediate.dense.weight', 'encoder.layer.10.output.LayerNorm.bias', 'encoder.layer.10.output.LayerNorm.weight', 'encoder.layer.10.output.dense.bias', 'encoder.layer.10.output.dense.weight', 'encoder.layer.11.attention.output.LayerNorm.bias', 'encoder.layer.11.attention.output.LayerNorm.weight', 'encoder.layer.11.attention.output.dense.bias', 'encoder.layer.11.attention.output.dense.weight', 'encoder.layer.11.attention.self.key.bias', 'encoder.layer.11.attention.self.key.weight', 'encoder.layer.11.attention.self.query.bias', 'encoder.layer.11.attention.self.query.weight', 'encoder.layer.11.attention.self.value.bias', 'encoder.layer.11.attention.self.value.weight', 'encoder.layer.11.intermediate.dense.bias', 'encoder.layer.11.intermediate.dense.weight', 'encoder.layer.11.output.LayerNorm.bias', 'encoder.layer.11.output.LayerNorm.weight', 'encoder.layer.11.output.dense.bias', 'encoder.layer.11.output.dense.weight', 'encoder.layer.2.attention.output.LayerNorm.bias', 'encoder.layer.2.attention.output.LayerNorm.weight', 'encoder.layer.2.attention.output.dense.bias', 'encoder.layer.2.attention.output.dense.weight', 'encoder.layer.2.attention.self.key.bias', 'encoder.layer.2.attention.self.key.weight', 'encoder.layer.2.attention.self.query.bias', 'encoder.layer.2.attention.self.query.weight', 'encoder.layer.2.attention.self.value.bias', 'encoder.layer.2.attention.self.value.weight', 'encoder.layer.2.intermediate.dense.bias', 'encoder.layer.2.intermediate.dense.weight', 'encoder.layer.2.output.LayerNorm.bias', 'encoder.layer.2.output.LayerNorm.weight', 'encoder.layer.2.output.dense.bias', 'encoder.layer.2.output.dense.weight', 'encoder.layer.3.attention.output.LayerNorm.bias', 'encoder.layer.3.attention.output.LayerNorm.weight', 'encoder.layer.3.attention.output.dense.bias', 'encoder.layer.3.attention.output.dense.weight', 'encoder.layer.3.attention.self.key.bias', 'encoder.layer.3.attention.self.key.weight', 'encoder.layer.3.attention.self.query.bias', 'encoder.layer.3.attention.self.query.weight', 'encoder.layer.3.attention.self.value.bias', 'encoder.layer.3.attention.self.value.weight', 'encoder.layer.3.intermediate.dense.bias', 'encoder.layer.3.intermediate.dense.weight', 'encoder.layer.3.output.LayerNorm.bias', 'encoder.layer.3.output.LayerNorm.weight', 'encoder.layer.3.output.dense.bias', 'encoder.layer.3.output.dense.weight', 'encoder.layer.4.attention.output.LayerNorm.bias', 'encoder.layer.4.attention.output.LayerNorm.weight', 'encoder.layer.4.attention.output.dense.bias', 'encoder.layer.4.attention.output.dense.weight', 'encoder.layer.4.attention.self.key.bias', 'encoder.layer.4.attention.self.key.weight', 'encoder.layer.4.attention.self.query.bias', 'encoder.layer.4.attention.self.query.weight', 'encoder.layer.4.attention.self.value.bias', 'encoder.layer.4.attention.self.value.weight', 'encoder.layer.4.intermediate.dense.bias', 'encoder.layer.4.intermediate.dense.weight', 'encoder.layer.4.output.LayerNorm.bias', 'encoder.layer.4.output.LayerNorm.weight', 'encoder.layer.4.output.dense.bias', 'encoder.layer.4.output.dense.weight', 'encoder.layer.5.attention.output.LayerNorm.bias', 'encoder.layer.5.attention.output.LayerNorm.weight', 'encoder.layer.5.attention.output.dense.bias', 'encoder.layer.5.attention.output.dense.weight', 'encoder.layer.5.attention.self.key.bias', 'encoder.layer.5.attention.self.key.weight', 'encoder.layer.5.attention.self.query.bias', 'encoder.layer.5.attention.self.query.weight', 'encoder.layer.5.attention.self.value.bias', 'encoder.layer.5.attention.self.value.weight', 'encoder.layer.5.intermediate.dense.bias', 'encoder.layer.5.intermediate.dense.weight', 'encoder.layer.5.output.LayerNorm.bias', 'encoder.layer.5.output.LayerNorm.weight', 'encoder.layer.5.output.dense.bias', 'encoder.layer.5.output.dense.weight', 'encoder.layer.6.attention.output.LayerNorm.bias', 'encoder.layer.6.attention.output.LayerNorm.weight', 'encoder.layer.6.attention.output.dense.bias', 'encoder.layer.6.attention.output.dense.weight', 'encoder.layer.6.attention.self.key.bias', 'encoder.layer.6.attention.self.key.weight', 'encoder.layer.6.attention.self.query.bias', 'encoder.layer.6.attention.self.query.weight', 'encoder.layer.6.attention.self.value.bias', 'encoder.layer.6.attention.self.value.weight', 'encoder.layer.6.intermediate.dense.bias', 'encoder.layer.6.intermediate.dense.weight', 'encoder.layer.6.output.LayerNorm.bias', 'encoder.layer.6.output.LayerNorm.weight', 'encoder.layer.6.output.dense.bias', 'encoder.layer.6.output.dense.weight', 'encoder.layer.7.attention.output.LayerNorm.bias', 'encoder.layer.7.attention.output.LayerNorm.weight', 'encoder.layer.7.attention.output.dense.bias', 'encoder.layer.7.attention.output.dense.weight', 'encoder.layer.7.attention.self.key.bias', 'encoder.layer.7.attention.self.key.weight', 'encoder.layer.7.attention.self.query.bias', 'encoder.layer.7.attention.self.query.weight', 'encoder.layer.7.attention.self.value.bias', 'encoder.layer.7.attention.self.value.weight', 'encoder.layer.7.intermediate.dense.bias', 'encoder.layer.7.intermediate.dense.weight', 'encoder.layer.7.output.LayerNorm.bias', 'encoder.layer.7.output.LayerNorm.weight', 'encoder.layer.7.output.dense.bias', 'encoder.layer.7.output.dense.weight', 'encoder.layer.8.attention.output.LayerNorm.bias', 'encoder.layer.8.attention.output.LayerNorm.weight', 'encoder.layer.8.attention.output.dense.bias', 'encoder.layer.8.attention.output.dense.weight', 'encoder.layer.8.attention.self.key.bias', 'encoder.layer.8.attention.self.key.weight', 'encoder.layer.8.attention.self.query.bias', 'encoder.layer.8.attention.self.query.weight', 'encoder.layer.8.attention.self.value.bias', 'encoder.layer.8.attention.self.value.weight', 'encoder.layer.8.intermediate.dense.bias', 'encoder.layer.8.intermediate.dense.weight', 'encoder.layer.8.output.LayerNorm.bias', 'encoder.layer.8.output.LayerNorm.weight', 'encoder.layer.8.output.dense.bias', 'encoder.layer.8.output.dense.weight', 'encoder.layer.9.attention.output.LayerNorm.bias', 'encoder.layer.9.attention.output.LayerNorm.weight', 'encoder.layer.9.attention.output.dense.bias', 'encoder.layer.9.attention.output.dense.weight', 'encoder.layer.9.attention.self.key.bias', 'encoder.layer.9.attention.self.key.weight', 'encoder.layer.9.attention.self.query.bias', 'encoder.layer.9.attention.self.query.weight', 'encoder.layer.9.attention.self.value.bias', 'encoder.layer.9.attention.self.value.weight', 'encoder.layer.9.intermediate.dense.bias', 'encoder.layer.9.intermediate.dense.weight', 'encoder.layer.9.output.LayerNorm.bias', 'encoder.layer.9.output.LayerNorm.weight', 'encoder.layer.9.output.dense.bias', 'encoder.layer.9.output.dense.weight', 'pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "[27-11-2024 02:30:11] DEBUG: https://huggingface.co:443 \"HEAD /bert-base-uncased/resolve/main/config.json HTTP/11\" 200 0\n",
      "[27-11-2024 02:30:11] DEBUG: https://huggingface.co:443 \"HEAD /bert-base-uncased/resolve/main/config.json HTTP/11\" 200 0\n"
     ]
    }
   ],
   "source": [
    "model_path = '/home/ec2-user/SageMaker/bert_model/model_best.pt'\n",
    "model = torch.load(model_path)\n",
    "\n",
    "config_path = '/home/ec2-user/SageMaker/bert_model/config.json'\n",
    "config = transformers.BertConfig.from_json_file(config_path)\n",
    "\n",
    "bert_model_1 = transformers.AutoModel.from_pretrained(model_path, config=config)\n",
    "\n",
    "bert_model_2 = transformers.BertModel.from_pretrained(\"bert-base-uncased\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "bert_model_1 = bert_model_1.cuda()\n",
    "bert_model_2 = bert_model_2.cuda()\n",
    "\n",
    "ori_model1 = inference_model(bert_model_1, args)\n",
    "ori_model2 = inference_model(bert_model_2, args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model_1 = ori_model1\n",
    "model_2 = ori_model2\n",
    "\n",
    "model_1 = model_1.cuda()\n",
    "model_2 = model_2.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[27-11-2024 02:30:12] DEBUG: Starting new HTTPS connection (1): ip-172-16-95-62.ap-south-1.compute.internal:8443\n",
      "[27-11-2024 02:30:12] ERROR: Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\n",
      "[27-11-2024 02:30:12] DEBUG: Starting new HTTPS connection (1): api.wandb.ai:443\n",
      "[27-11-2024 02:30:12] DEBUG: https://api.wandb.ai:443 \"POST /graphql HTTP/11\" 200 1997\n",
      "[27-11-2024 02:30:12] DEBUG: https://api.wandb.ai:443 \"POST /graphql HTTP/11\" 200 374\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mshubham-kumar1\u001b[0m (\u001b[33mshubham-kumar1-shl\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /home/ec2-user/.netrc\n",
      "[27-11-2024 02:30:12] DEBUG: Starting new HTTPS connection (1): api.wandb.ai:443\n",
      "[27-11-2024 02:30:13] DEBUG: https://api.wandb.ai:443 \"POST /graphql HTTP/11\" 200 1997\n",
      "[27-11-2024 02:30:13] DEBUG: https://api.wandb.ai:443 \"POST /graphql HTTP/11\" 200 374\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.18.7"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/ec2-user/SageMaker/Grammar_modelling/wandb/run-20241127_023013-b7b9zoh7</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/shubham-kumar1-shl/Grammar_modelling/runs/b7b9zoh7' target=\"_blank\">Bert_co_teaching_1e4</a></strong> to <a href='https://wandb.ai/shubham-kumar1-shl/Grammar_modelling' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/shubham-kumar1-shl/Grammar_modelling' target=\"_blank\">https://wandb.ai/shubham-kumar1-shl/Grammar_modelling</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/shubham-kumar1-shl/Grammar_modelling/runs/b7b9zoh7' target=\"_blank\">https://wandb.ai/shubham-kumar1-shl/Grammar_modelling/runs/b7b9zoh7</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages/transformers/optimization.py:591: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "1252it [52:41,  2.53s/it]\n",
      "[27-11-2024 03:22:55] INFO: Epoch: 0,Model Loss:1.9041461065506782, Loss Model1: 0.9592586560680653, Loss Model2: 0.944887450482613, Forget Rate: 0.2\n",
      "[27-11-2024 03:22:55] INFO: Start eval for Model 1!\n",
      "68it [00:28,  2.38it/s]\n",
      "[27-11-2024 03:23:24] INFO: {'Prearson Corr': -0.008571035167081922, 'Spearman Corr': 0.000946886729483648, 'Eval Loss': 1.3891738395634985}\n",
      "[27-11-2024 03:23:24] INFO: Start eval for Model 2!\n",
      "68it [00:28,  2.37it/s]\n",
      "[27-11-2024 03:23:53] INFO: {'Prearson Corr': 0.4084534936163933, 'Spearman Corr': 0.41493437381684706, 'Eval Loss': 1.2912103416471448}\n",
      "1252it [52:42,  2.53s/it]\n",
      "[27-11-2024 04:16:36] INFO: Epoch: 1,Model Loss:1.807532645613193, Loss Model1: 0.9058576483672229, Loss Model2: 0.9016749972459702, Forget Rate: 0.2\n",
      "[27-11-2024 04:16:36] INFO: Start eval for Model 1!\n",
      "68it [00:28,  2.38it/s]\n",
      "[27-11-2024 04:17:05] INFO: {'Prearson Corr': 0.1319728148230159, 'Spearman Corr': 0.1463363949953545, 'Eval Loss': 1.7286005455723192}\n",
      "[27-11-2024 04:17:05] INFO: Start eval for Model 2!\n",
      "68it [00:28,  2.38it/s]\n",
      "[27-11-2024 04:17:33] INFO: {'Prearson Corr': -0.0066896290813779555, 'Spearman Corr': 0.008379540259500252, 'Eval Loss': 1.6897492680072863}\n",
      "1252it [52:42,  2.53s/it]\n",
      "[27-11-2024 05:10:17] INFO: Epoch: 2,Model Loss:1.7925478578193055, Loss Model1: 0.8974411010274047, Loss Model2: 0.8951067567919009, Forget Rate: 0.2\n",
      "[27-11-2024 05:10:17] INFO: Start eval for Model 1!\n",
      "68it [00:28,  2.38it/s]\n",
      "[27-11-2024 05:10:45] INFO: {'Prearson Corr': 0.0018761784409135262, 'Spearman Corr': 0.030698003117975606, 'Eval Loss': 1.9926729906492866}\n",
      "[27-11-2024 05:10:45] INFO: Start eval for Model 2!\n",
      "68it [00:28,  2.38it/s]\n",
      "[27-11-2024 05:11:14] INFO: {'Prearson Corr': -0.0019692921524401656, 'Spearman Corr': -0.025386790957250445, 'Eval Loss': 1.919558337305178}\n",
      "1252it [52:43,  2.53s/it]\n",
      "[27-11-2024 06:03:58] INFO: Epoch: 3,Model Loss:1.7697588043531385, Loss Model1: 0.8853673870934132, Loss Model2: 0.8843914172597254, Forget Rate: 0.2\n",
      "[27-11-2024 06:03:58] INFO: Start eval for Model 1!\n",
      "68it [00:28,  2.39it/s]\n",
      "[27-11-2024 06:04:27] INFO: {'Prearson Corr': -0.026120553436584978, 'Spearman Corr': -0.03167707228535376, 'Eval Loss': 1.656800835105791}\n",
      "[27-11-2024 06:04:27] INFO: Start eval for Model 2!\n",
      "68it [00:28,  2.37it/s]\n",
      "[27-11-2024 06:04:55] INFO: {'Prearson Corr': 0.03697515895163813, 'Spearman Corr': 0.040293494326902185, 'Eval Loss': 1.9044172390339047}\n",
      "1252it [52:42,  2.53s/it]\n",
      "[27-11-2024 06:57:39] INFO: Epoch: 4,Model Loss:1.754298864226467, Loss Model1: 0.8787779193026379, Loss Model2: 0.8755209449238289, Forget Rate: 0.2\n",
      "[27-11-2024 06:57:39] INFO: Start eval for Model 1!\n",
      "68it [00:28,  2.38it/s]\n",
      "[27-11-2024 06:58:08] INFO: {'Prearson Corr': 0.16047194771565707, 'Spearman Corr': 0.147529418313903, 'Eval Loss': 1.8519370598451506}\n",
      "[27-11-2024 06:58:08] INFO: Start eval for Model 2!\n",
      "68it [00:28,  2.37it/s]\n",
      "[27-11-2024 06:58:37] INFO: {'Prearson Corr': 0.21395440510362762, 'Spearman Corr': 0.19922875646151947, 'Eval Loss': 1.7548859978964113}\n",
      "1252it [52:46,  2.53s/it]\n",
      "[27-11-2024 09:38:52] INFO: Epoch: 7,Model Loss:1.7269329917364227, Loss Model1: 0.8663460177119476, Loss Model2: 0.8605869740244751, Forget Rate: 0.2\n",
      "[27-11-2024 09:38:52] INFO: Start eval for Model 1!\n",
      "68it [00:28,  2.36it/s]\n",
      "[27-11-2024 09:39:21] INFO: {'Prearson Corr': 0.06896488184555032, 'Spearman Corr': 0.06032694849219448, 'Eval Loss': 1.4085591187053066}\n",
      "[27-11-2024 09:39:21] INFO: Start eval for Model 2!\n",
      "68it [00:28,  2.38it/s]\n",
      "[27-11-2024 09:39:50] INFO: {'Prearson Corr': 0.19850918164326656, 'Spearman Corr': 0.24882146932034713, 'Eval Loss': 1.3169361788009946}\n",
      "1252it [52:48,  2.53s/it]\n",
      "[27-11-2024 10:32:39] INFO: Epoch: 8,Model Loss:1.7269092937842154, Loss Model1: 0.8665072734014422, Loss Model2: 0.8604020203827732, Forget Rate: 0.2\n",
      "[27-11-2024 10:32:39] INFO: Start eval for Model 1!\n",
      "68it [00:28,  2.36it/s]\n",
      "[27-11-2024 10:33:08] INFO: {'Prearson Corr': 0.09523103159217988, 'Spearman Corr': 0.11022535339171491, 'Eval Loss': 1.1974614583459708}\n",
      "[27-11-2024 10:33:08] INFO: Start eval for Model 2!\n",
      "68it [00:28,  2.38it/s]\n",
      "[27-11-2024 10:33:37] INFO: {'Prearson Corr': 0.2635155509963125, 'Spearman Corr': 0.2678973829795749, 'Eval Loss': 1.2227270993004151}\n",
      "230it [09:43,  2.53s/it]"
     ]
    }
   ],
   "source": [
    "wandb.login(key=args.wandb_key)\n",
    "wandb.init(project=args.wandb_proj_name, config=args, name=args.wandb_run_name)\n",
    "\n",
    "# train_model_co_teaching_plus(model_1,model_2, args, trainset_reader, validset_reader)\n",
    "train_model_co_teaching(model_1,model_2, args, trainset_reader, validset_reader)\n",
    "\n",
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_pytorch_p310",
   "language": "python",
   "name": "conda_pytorch_p310"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
