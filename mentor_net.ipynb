{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torch in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (2.2.2)\n",
      "Requirement already satisfied: filelock in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from torch) (3.16.1)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from torch) (4.12.2)\n",
      "Requirement already satisfied: sympy in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from torch) (1.13.3)\n",
      "Requirement already satisfied: networkx in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from torch) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from torch) (3.1.4)\n",
      "Requirement already satisfied: fsspec in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from torch) (2024.10.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from jinja2->torch) (3.0.2)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from sympy->torch) (1.3.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: transformers in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (4.46.3)\n",
      "Requirement already satisfied: filelock in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from transformers) (3.16.1)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.23.2 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from transformers) (0.26.2)\n",
      "Requirement already satisfied: numpy>=1.17 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from transformers) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from transformers) (21.3)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from transformers) (6.0.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from transformers) (2024.11.6)\n",
      "Requirement already satisfied: requests in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from transformers) (2.32.3)\n",
      "Requirement already satisfied: tokenizers<0.21,>=0.20 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from transformers) (0.20.3)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from transformers) (0.4.5)\n",
      "Requirement already satisfied: tqdm>=4.27 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from transformers) (4.66.5)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.23.2->transformers) (2024.10.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.23.2->transformers) (4.12.2)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from packaging>=20.0->transformers) (3.2.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from requests->transformers) (3.4.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from requests->transformers) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from requests->transformers) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from requests->transformers) (2024.8.30)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: wandb in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (0.18.7)\n",
      "Requirement already satisfied: click!=8.0.0,>=7.1 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from wandb) (8.1.7)\n",
      "Requirement already satisfied: docker-pycreds>=0.4.0 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from wandb) (0.4.0)\n",
      "Requirement already satisfied: gitpython!=3.1.29,>=1.0.0 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from wandb) (3.1.43)\n",
      "Requirement already satisfied: platformdirs in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from wandb) (4.3.6)\n",
      "Requirement already satisfied: protobuf!=4.21.0,!=5.28.0,<6,>=3.19.0 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from wandb) (4.25.5)\n",
      "Requirement already satisfied: psutil>=5.0.0 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from wandb) (6.0.0)\n",
      "Requirement already satisfied: pyyaml in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from wandb) (6.0.2)\n",
      "Requirement already satisfied: requests<3,>=2.0.0 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from wandb) (2.32.3)\n",
      "Requirement already satisfied: sentry-sdk>=2.0.0 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from wandb) (2.18.0)\n",
      "Requirement already satisfied: setproctitle in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from wandb) (1.3.4)\n",
      "Requirement already satisfied: setuptools in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from wandb) (75.2.0)\n",
      "Requirement already satisfied: typing-extensions<5,>=4.4 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from wandb) (4.12.2)\n",
      "Requirement already satisfied: six>=1.4.0 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from docker-pycreds>=0.4.0->wandb) (1.16.0)\n",
      "Requirement already satisfied: gitdb<5,>=4.0.1 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from gitpython!=3.1.29,>=1.0.0->wandb) (4.0.11)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from requests<3,>=2.0.0->wandb) (3.4.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from requests<3,>=2.0.0->wandb) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from requests<3,>=2.0.0->wandb) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from requests<3,>=2.0.0->wandb) (2024.8.30)\n",
      "Requirement already satisfied: smmap<6,>=3.0.1 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from gitdb<5,>=4.0.1->gitpython!=3.1.29,>=1.0.0->wandb) (5.0.1)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install torch\n",
    "%pip install transformers\n",
    "%pip install wandb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cache Space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: LC_ALL=C.UTF-8\n",
      "env: LANG=C.UTF-8\n",
      "env: TRANSFORMERS_CACHE=/home/ec2-user/SageMaker/cache\n",
      "env: HF_DATASETS_CACHE=/home/ec2-user/SageMaker/cache\n",
      "env: CUDA_LAUNCH_BLOCKING=1\n"
     ]
    }
   ],
   "source": [
    "%env LC_ALL=C.UTF-8\n",
    "%env LANG=C.UTF-8\n",
    "%env TRANSFORMERS_CACHE=/home/ec2-user/SageMaker/cache\n",
    "%env HF_DATASETS_CACHE=/home/ec2-user/SageMaker/cache\n",
    "%env CUDA_LAUNCH_BLOCKING=1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import Lib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages/transformers/utils/hub.py:128: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import yaml\n",
    "import random, os\n",
    "import argparse\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from tqdm import tqdm\n",
    "from torch.autograd import Variable\n",
    "from tqdm import tqdm\n",
    "import transformers\n",
    "from torch.nn import NLLLoss\n",
    "import logging\n",
    "import json\n",
    "import torch.nn as nn\n",
    "import re\n",
    "from torch.autograd import Variable\n",
    "import wandb\n",
    "from pathlib import Path\n",
    "import math\n",
    "\n",
    "\n",
    "logger = logging.getLogger(__name__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def reform_label(tokens, label, tokenizer, max_seq_length):\n",
    "    new_tokens = list()\n",
    "    new_label = list()\n",
    "    for step, token in enumerate(tokens[:-1]):\n",
    "        split_token = tokenizer.tokenize(token)\n",
    "        if len(split_token) > 0:\n",
    "            new_tokens.extend(split_token)\n",
    "            \n",
    "    new_label.append(label)\n",
    "    new_tokens = new_tokens[:max_seq_length] + ['EOS']\n",
    "    \n",
    "    return new_tokens, new_label\n",
    "\n",
    "def tok2int_sent(example, tokenizer, max_seq_length):\n",
    "    src_tokens = example[0]\n",
    "    src_label = example[1]\n",
    "\n",
    "    src_tokens, src_label = reform_label(src_tokens, src_label, tokenizer, max_seq_length)\n",
    "\n",
    "    tokens = src_tokens\n",
    "    tokens = [\"[CLS]\"] + tokens\n",
    "    label = src_label\n",
    "    input_seg = [1] * len(tokens)\n",
    "\n",
    "    input_ids = tokenizer.convert_tokens_to_ids(tokens)\n",
    "    input_mask = [1] * len(input_ids)\n",
    "\n",
    "    max_len = max_seq_length * 2 + 3\n",
    "    padding = [0] * (max_len - len(input_ids))\n",
    "    input_ids += padding\n",
    "    input_mask += padding\n",
    "    input_seg += padding\n",
    "\n",
    "    return input_ids, input_mask, input_seg, label\n",
    "\n",
    "def tok2int_list(data, tokenizer, max_seq_length):\n",
    "    inps = list()\n",
    "    msks = list()\n",
    "    segs = list()\n",
    "    labs = list()\n",
    "    for examples in data:\n",
    "        input_ids, input_mask, input_seg, labels = tok2int_sent(examples, tokenizer, max_seq_length)\n",
    "        inps.append(input_ids)\n",
    "        msks.append(input_mask)\n",
    "        segs.append(input_seg)\n",
    "        labs.append(labels)\n",
    "        \n",
    "    return inps, msks, segs, labs\n",
    "\n",
    "\n",
    "class DataLoader(object):\n",
    "    ''' For data iteration '''\n",
    "\n",
    "    def __init__(self, data_path, tokenizer, args, test=False, batch_size=64):\n",
    "        self.cuda = args.cuda\n",
    "        self.batch_size = batch_size\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_len = args.max_len\n",
    "        self.data_path = data_path\n",
    "        self.test = test\n",
    "        self.src_flag = args.src_flag\n",
    "        self.hyp_flag = args.hyp_flag\n",
    "        examples = self.read_file(data_path)\n",
    "        self.examples = examples\n",
    "        self.total_num = len(examples)\n",
    "        if self.test:\n",
    "            self.total_step = np.ceil(self.total_num * 1.0 / batch_size)\n",
    "        else:\n",
    "            self.total_step = self.total_num / batch_size\n",
    "            self.shuffle()\n",
    "        self.step = 0\n",
    "    \n",
    "    def read_file(self, data_path):\n",
    "        data_list = list()\n",
    "        data = pd.read_csv(data_path)\n",
    "        data = data[data['grammarScore']>0]\n",
    "        data.dropna(inplace=True)\n",
    "\n",
    "        for _ , row in data.iterrows():\n",
    "            example = list()\n",
    "            line = row['transcript']\n",
    "            src_token = line.split()\n",
    "            src_label = float(row['grammarScore'])\n",
    "            example.append(src_token)\n",
    "            example.append(src_label)\n",
    "            data_list.append(example)\n",
    "        return data_list\n",
    "\n",
    "\n",
    "    def shuffle(self):\n",
    "        np.random.shuffle(self.examples)\n",
    "\n",
    "    def __iter__(self):\n",
    "        return self\n",
    "\n",
    "    def __next__(self):\n",
    "        return self.next()\n",
    "\n",
    "    def __len__(self):\n",
    "        return self._n_batch\n",
    "\n",
    "    def next(self):\n",
    "        ''' Get the next batch '''\n",
    "        if self.step < self.total_step:\n",
    "            examples = self.examples[self.step * self.batch_size : (self.step+1)*self.batch_size]\n",
    "            \n",
    "            inp, msk, seg, score = tok2int_list(examples, self.tokenizer, self.max_len)\n",
    "\n",
    "            inp_tensor = Variable(\n",
    "                torch.LongTensor(inp))\n",
    "            msk_tensor = Variable(\n",
    "                torch.LongTensor(msk))\n",
    "            seg_tensor = Variable(\n",
    "                torch.LongTensor(seg))\n",
    "            score_tensor = Variable(\n",
    "                torch.LongTensor(score))\n",
    "\n",
    "            if self.cuda:\n",
    "                inp_tensor = inp_tensor.cuda()\n",
    "                msk_tensor = msk_tensor.cuda()\n",
    "                seg_tensor = seg_tensor.cuda()\n",
    "                score_tensor = score_tensor.cuda()\n",
    "\n",
    "            self.step += 1\n",
    "            return inp_tensor, msk_tensor, seg_tensor, score_tensor\n",
    "\n",
    "        else:\n",
    "            self.step = 0\n",
    "            if not self.test:\n",
    "                self.shuffle()\n",
    "            raise StopIteration()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from scipy.stats import pearsonr,spearmanr\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import numpy as np\n",
    "\n",
    "def eval_result(predicts, labels):\n",
    "    spearman_corr, _ = spearmanr(predicts, labels)\n",
    "    corr, _ = pearsonr(predicts, labels)\n",
    "    res = {\"Prearson Corr\":corr,\"Spearman Corr\":spearman_corr,\"Eval Loss\":np.sqrt(mean_squared_error(predicts, labels))}\n",
    "    return res\n",
    "\n",
    "def eval_model(model, validset_reader):\n",
    "    model.eval()\n",
    "    predicts = list()\n",
    "    labels = list()\n",
    "    with torch.no_grad():\n",
    "        for step, (inp_tensor, msk_tensor, seg_tensor, score_tensor) in tqdm(enumerate(validset_reader)):\n",
    "            # print(\"Eval Input --\",inp_tensor)\n",
    "            # print(\"Eval msk_tensor --\",msk_tensor)\n",
    "            # print(\"Eval seg_tensor --\",seg_tensor)\n",
    "            \n",
    "            score_tensor = score_tensor.to(torch.float)\n",
    "            prob = model(inp_tensor, msk_tensor, seg_tensor)\n",
    "\n",
    "            # print(\"Eval output---->\",prob.view(-1).tolist())\n",
    "            \n",
    "            predict = prob.type_as(score_tensor).view(-1).tolist()\n",
    "            score = score_tensor.view(-1).tolist()\n",
    "            predicts.extend(predict)\n",
    "            labels.extend(score)\n",
    "            \n",
    "        results = eval_result(predicts, labels)\n",
    "            \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# list_net_loss\n",
    "\n",
    "def listnet_loss(y_i, z_i):\n",
    "    P_y_i = F.softmax(y_i.float(), dim=0)\n",
    "    P_z_i = F.softmax(z_i.float(), dim=0)\n",
    "    return - torch.sum(P_y_i * torch.log(P_z_i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def train_model_mentor_net(mentor_model, student_model, args, trainset_reader, validset_reader):\n",
    "    saved_checkpoints1 = []\n",
    "    saved_checkpoints2 = []\n",
    "    \n",
    "    save_path = args.outdir\n",
    "    running_loss = 0.0\n",
    "    meta_loss_mentor = 0.0\n",
    "\n",
    "    t_total = int(trainset_reader.total_step / args.gradient_accumulation_steps * args.num_train_epochs)\n",
    "\n",
    "    optimizer_mentor = transformers.AdamW(mentor_model.parameters(), lr=args.learning_rate, eps=1e-8)\n",
    "    optimizer_student = transformers.AdamW(student_model.parameters(), lr=args.learning_rate, eps=1e-8)\n",
    "\n",
    "    scheduler_student = transformers.get_linear_schedule_with_warmup(\n",
    "        optimizer_student, num_warmup_steps=0, num_training_steps=t_total\n",
    "    )\n",
    "\n",
    "    global_step = 0\n",
    "    for epoch in range(int(args.num_train_epochs)):\n",
    "        optimizer_mentor.zero_grad()\n",
    "        optimizer_student.zero_grad()\n",
    "        mentor_model.train()\n",
    "        student_model.train()\n",
    "\n",
    "        for inp_tensor, msk_tensor, seg_tensor, score_tensor in tqdm(trainset_reader):\n",
    "            \n",
    "            pred_student = student_model(inp_tensor, msk_tensor, seg_tensor).view(-1)\n",
    "\n",
    "            score_tensor = score_tensor.view(-1).to(torch.float)\n",
    "    \n",
    "            loss_student = F.mse_loss(pred_student, score_tensor, reduction='none')\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                sample_features = loss_student\n",
    "                weights = mentor_model(sample_features)\n",
    "            \n",
    "            weighted_loss_student = (weights * loss_student).mean()\n",
    "    \n",
    "            running_loss += weighted_loss_student.item()\n",
    "            \n",
    "            weighted_loss_student.backward()\n",
    "            \n",
    "            global_step += 1\n",
    "            if global_step % args.gradient_accumulation_steps == 0:\n",
    "                optimizer_student.step()\n",
    "                scheduler_student.step()\n",
    "                optimizer_student.zero_grad()\n",
    "\n",
    "            \n",
    "        mentor_model.train()\n",
    "        for meta_inp_tensor, meta_msk_tensor, meta_seg_tensor, meta_score_tensor in validset_reader:\n",
    "            with torch.no_grad():\n",
    "                meta_student_output = student_model(meta_inp_tensor, meta_msk_tensor, meta_seg_tensor)  \n",
    "                meta_pred_score = meta_student_output.view(-1)\n",
    "                meta_score_tensor = meta_score_tensor.view(-1)\n",
    "            \n",
    "            meta_loss = F.mse_loss(meta_pred_score, meta_score_tensor.to(torch.float))\n",
    "            meta_loss_mentor += meta_loss\n",
    "            \n",
    "            optimizer_mentor.zero_grad()\n",
    "            meta_loss.backward()\n",
    "            optimizer_mentor.step()\n",
    "            \n",
    "        logger.info('Epoch: {}, Student Loss:{}, Meta Loss:{}, Loss2: {}, LR1: {}, LR2: {}'.format(epoch, running_loss / global_step,running_loss / global_step, meta_loss_mentor / global_step, scheduler_student.get_last_lr()[0]))\n",
    "\n",
    "        train_res = {\n",
    "            \"Student Train Loss\": running_loss / global_step,\n",
    "            \"Mentor Meta Loss\": meta_loss_mentor / global_step,\n",
    "            \"Learning\": scheduler_student.get_last_lr()[0],\n",
    "        }\n",
    "\n",
    "        logger.info('Start eval for Model 1!')\n",
    "        result_dict1 = eval_model(student_model, validset_reader)\n",
    "        logger.info(result_dict1)\n",
    "        \n",
    "        logger.info('Start eval for Model 2!')\n",
    "        result_dict2 = eval_model(mentor_model, validset_reader)\n",
    "        logger.info(result_dict2)\n",
    "\n",
    "        train_res.update({\"Student Model Validation\": result_dict1, \"Mentor Model Validation\": result_dict2})\n",
    "        wandb.log(train_res)\n",
    "        \n",
    "        check_point_path1 = save_path + f\"/student_mode_{epoch}_best.pt\"\n",
    "        check_point_path2 = save_path + f\"/mentor_model_{epoch}_best.pt\"\n",
    "\n",
    "        torch.save({'epoch': epoch,\n",
    "                    'model': student_model.state_dict()},check_point_path1)\n",
    "        torch.save({'epoch': epoch,\n",
    "            'model': mentor_model.state_dict()},check_point_path2)\n",
    "        \n",
    "        saved_checkpoints1.append(check_point_path1)\n",
    "        saved_checkpoints2.append(check_point_path2)\n",
    "\n",
    "        if len(saved_checkpoints1) > args.max_model_save:\n",
    "            old_checkpoint1 = saved_checkpoints1.pop(0)\n",
    "            old_checkpoint2 = saved_checkpoints2.pop(0)\n",
    "            \n",
    "            if os.path.exists(old_checkpoint1):\n",
    "                os.remove(old_checkpoint1)\n",
    "                \n",
    "            if os.path.exists(old_checkpoint2):\n",
    "                os.remove(old_checkpoint2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Attention Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class inference_model(nn.Module):\n",
    "    def __init__(self, bert_model, args):\n",
    "        super(inference_model, self).__init__()\n",
    "        self.bert_hidden_dim = args.bert_hidden_dim\n",
    "        self.pred_model = bert_model\n",
    "        self.model_name = args.bert_pretrain\n",
    "        self.max_len = args.max_len * 2 + 3\n",
    "        self.proj_hidden = nn.Linear(self.bert_hidden_dim, 1)\n",
    "\n",
    "\n",
    "    def forward(self, inp_tensor, msk_tensor, seg_tensor,score_flag=True):\n",
    "        inp_tensor = inp_tensor.view(-1, self.max_len)\n",
    "        msk_tensor = msk_tensor.view(-1, self.max_len)\n",
    "        seg_tensor = seg_tensor.view(-1, self.max_len)\n",
    "        \n",
    "        if \"bert\" in self.model_name.lower():\n",
    "            outputs = self.pred_model(inp_tensor, msk_tensor, seg_tensor)\n",
    "            \n",
    "        else:\n",
    "            BaseException (\"Not implement!\")\n",
    "        \n",
    "        pred_score = self.proj_hidden(outputs.pooler_output)\n",
    "\n",
    "        return pred_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MENOTR_MODEL(nn.Module):\n",
    "    def __init__(self,args):\n",
    "        super(MENOTR_MODEL, self).__init__()\n",
    "        self.hidden_dim = args.train_batch_size\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(self.hidden_dim, self.hidden_dim *2),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(self.hidden_dim *2,self.hidden_dim),\n",
    "            nn.Softmax(dim=-1)\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.fc(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[26-11-2024 02:47:45] INFO: <__main__.Config object at 0x7fbd74397f10>\n",
      "[26-11-2024 02:47:45] DEBUG: Starting new HTTPS connection (1): huggingface.co:443\n",
      "[26-11-2024 02:47:45] DEBUG: https://huggingface.co:443 \"HEAD /bert-base-uncased/resolve/main/tokenizer_config.json HTTP/11\" 200 0\n",
      "[26-11-2024 02:47:45] INFO: Start training!\n",
      "[26-11-2024 02:47:45] INFO: loading training set\n",
      "[26-11-2024 02:47:47] INFO: loading validation set\n",
      "[26-11-2024 02:47:47] INFO: initializing estimator model\n"
     ]
    }
   ],
   "source": [
    "class Config:\n",
    "    def __init__(self, **entries):\n",
    "        self.__dict__.update(entries)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    config_file = 'config.yaml'\n",
    "    with open(config_file, \"r\") as ymlfile:\n",
    "        config_dict = yaml.load(ymlfile, Loader=yaml.FullLoader)\n",
    "\n",
    "    args = Config(**config_dict)\n",
    "\n",
    "    if not os.path.exists(args.outdir):\n",
    "        Path(args.outdir).mkdir(parents=True, exist_ok=True)\n",
    "        \n",
    "    handlers = [logging.FileHandler(os.path.abspath(args.outdir) + '/train_log.txt'), logging.StreamHandler()]\n",
    "    logging.basicConfig(format='[%(asctime)s] %(levelname)s: %(message)s', level=logging.DEBUG,\n",
    "                        datefmt='%d-%m-%Y %H:%M:%S', handlers=handlers)\n",
    "    logger.info(args)\n",
    "\n",
    "    tokenizer = transformers.BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "    \n",
    "    logger.info('Start training!')\n",
    "    logger.info(\"loading training set\")\n",
    "    trainset_reader = DataLoader(args.train_path, tokenizer, args, batch_size=args.train_batch_size)\n",
    "    logger.info(\"loading validation set\")\n",
    "    validset_reader = DataLoader(args.test_path, tokenizer, args, batch_size=args.valid_batch_size)\n",
    "    logger.info('initializing estimator model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertModel were not initialized from the model checkpoint at /home/ec2-user/SageMaker/bert_model/model_best.pt and are newly initialized: ['embeddings.LayerNorm.bias', 'embeddings.LayerNorm.weight', 'embeddings.position_embeddings.weight', 'embeddings.token_type_embeddings.weight', 'embeddings.word_embeddings.weight', 'encoder.layer.0.attention.output.LayerNorm.bias', 'encoder.layer.0.attention.output.LayerNorm.weight', 'encoder.layer.0.attention.output.dense.bias', 'encoder.layer.0.attention.output.dense.weight', 'encoder.layer.0.attention.self.key.bias', 'encoder.layer.0.attention.self.key.weight', 'encoder.layer.0.attention.self.query.bias', 'encoder.layer.0.attention.self.query.weight', 'encoder.layer.0.attention.self.value.bias', 'encoder.layer.0.attention.self.value.weight', 'encoder.layer.0.intermediate.dense.bias', 'encoder.layer.0.intermediate.dense.weight', 'encoder.layer.0.output.LayerNorm.bias', 'encoder.layer.0.output.LayerNorm.weight', 'encoder.layer.0.output.dense.bias', 'encoder.layer.0.output.dense.weight', 'encoder.layer.1.attention.output.LayerNorm.bias', 'encoder.layer.1.attention.output.LayerNorm.weight', 'encoder.layer.1.attention.output.dense.bias', 'encoder.layer.1.attention.output.dense.weight', 'encoder.layer.1.attention.self.key.bias', 'encoder.layer.1.attention.self.key.weight', 'encoder.layer.1.attention.self.query.bias', 'encoder.layer.1.attention.self.query.weight', 'encoder.layer.1.attention.self.value.bias', 'encoder.layer.1.attention.self.value.weight', 'encoder.layer.1.intermediate.dense.bias', 'encoder.layer.1.intermediate.dense.weight', 'encoder.layer.1.output.LayerNorm.bias', 'encoder.layer.1.output.LayerNorm.weight', 'encoder.layer.1.output.dense.bias', 'encoder.layer.1.output.dense.weight', 'encoder.layer.10.attention.output.LayerNorm.bias', 'encoder.layer.10.attention.output.LayerNorm.weight', 'encoder.layer.10.attention.output.dense.bias', 'encoder.layer.10.attention.output.dense.weight', 'encoder.layer.10.attention.self.key.bias', 'encoder.layer.10.attention.self.key.weight', 'encoder.layer.10.attention.self.query.bias', 'encoder.layer.10.attention.self.query.weight', 'encoder.layer.10.attention.self.value.bias', 'encoder.layer.10.attention.self.value.weight', 'encoder.layer.10.intermediate.dense.bias', 'encoder.layer.10.intermediate.dense.weight', 'encoder.layer.10.output.LayerNorm.bias', 'encoder.layer.10.output.LayerNorm.weight', 'encoder.layer.10.output.dense.bias', 'encoder.layer.10.output.dense.weight', 'encoder.layer.11.attention.output.LayerNorm.bias', 'encoder.layer.11.attention.output.LayerNorm.weight', 'encoder.layer.11.attention.output.dense.bias', 'encoder.layer.11.attention.output.dense.weight', 'encoder.layer.11.attention.self.key.bias', 'encoder.layer.11.attention.self.key.weight', 'encoder.layer.11.attention.self.query.bias', 'encoder.layer.11.attention.self.query.weight', 'encoder.layer.11.attention.self.value.bias', 'encoder.layer.11.attention.self.value.weight', 'encoder.layer.11.intermediate.dense.bias', 'encoder.layer.11.intermediate.dense.weight', 'encoder.layer.11.output.LayerNorm.bias', 'encoder.layer.11.output.LayerNorm.weight', 'encoder.layer.11.output.dense.bias', 'encoder.layer.11.output.dense.weight', 'encoder.layer.2.attention.output.LayerNorm.bias', 'encoder.layer.2.attention.output.LayerNorm.weight', 'encoder.layer.2.attention.output.dense.bias', 'encoder.layer.2.attention.output.dense.weight', 'encoder.layer.2.attention.self.key.bias', 'encoder.layer.2.attention.self.key.weight', 'encoder.layer.2.attention.self.query.bias', 'encoder.layer.2.attention.self.query.weight', 'encoder.layer.2.attention.self.value.bias', 'encoder.layer.2.attention.self.value.weight', 'encoder.layer.2.intermediate.dense.bias', 'encoder.layer.2.intermediate.dense.weight', 'encoder.layer.2.output.LayerNorm.bias', 'encoder.layer.2.output.LayerNorm.weight', 'encoder.layer.2.output.dense.bias', 'encoder.layer.2.output.dense.weight', 'encoder.layer.3.attention.output.LayerNorm.bias', 'encoder.layer.3.attention.output.LayerNorm.weight', 'encoder.layer.3.attention.output.dense.bias', 'encoder.layer.3.attention.output.dense.weight', 'encoder.layer.3.attention.self.key.bias', 'encoder.layer.3.attention.self.key.weight', 'encoder.layer.3.attention.self.query.bias', 'encoder.layer.3.attention.self.query.weight', 'encoder.layer.3.attention.self.value.bias', 'encoder.layer.3.attention.self.value.weight', 'encoder.layer.3.intermediate.dense.bias', 'encoder.layer.3.intermediate.dense.weight', 'encoder.layer.3.output.LayerNorm.bias', 'encoder.layer.3.output.LayerNorm.weight', 'encoder.layer.3.output.dense.bias', 'encoder.layer.3.output.dense.weight', 'encoder.layer.4.attention.output.LayerNorm.bias', 'encoder.layer.4.attention.output.LayerNorm.weight', 'encoder.layer.4.attention.output.dense.bias', 'encoder.layer.4.attention.output.dense.weight', 'encoder.layer.4.attention.self.key.bias', 'encoder.layer.4.attention.self.key.weight', 'encoder.layer.4.attention.self.query.bias', 'encoder.layer.4.attention.self.query.weight', 'encoder.layer.4.attention.self.value.bias', 'encoder.layer.4.attention.self.value.weight', 'encoder.layer.4.intermediate.dense.bias', 'encoder.layer.4.intermediate.dense.weight', 'encoder.layer.4.output.LayerNorm.bias', 'encoder.layer.4.output.LayerNorm.weight', 'encoder.layer.4.output.dense.bias', 'encoder.layer.4.output.dense.weight', 'encoder.layer.5.attention.output.LayerNorm.bias', 'encoder.layer.5.attention.output.LayerNorm.weight', 'encoder.layer.5.attention.output.dense.bias', 'encoder.layer.5.attention.output.dense.weight', 'encoder.layer.5.attention.self.key.bias', 'encoder.layer.5.attention.self.key.weight', 'encoder.layer.5.attention.self.query.bias', 'encoder.layer.5.attention.self.query.weight', 'encoder.layer.5.attention.self.value.bias', 'encoder.layer.5.attention.self.value.weight', 'encoder.layer.5.intermediate.dense.bias', 'encoder.layer.5.intermediate.dense.weight', 'encoder.layer.5.output.LayerNorm.bias', 'encoder.layer.5.output.LayerNorm.weight', 'encoder.layer.5.output.dense.bias', 'encoder.layer.5.output.dense.weight', 'encoder.layer.6.attention.output.LayerNorm.bias', 'encoder.layer.6.attention.output.LayerNorm.weight', 'encoder.layer.6.attention.output.dense.bias', 'encoder.layer.6.attention.output.dense.weight', 'encoder.layer.6.attention.self.key.bias', 'encoder.layer.6.attention.self.key.weight', 'encoder.layer.6.attention.self.query.bias', 'encoder.layer.6.attention.self.query.weight', 'encoder.layer.6.attention.self.value.bias', 'encoder.layer.6.attention.self.value.weight', 'encoder.layer.6.intermediate.dense.bias', 'encoder.layer.6.intermediate.dense.weight', 'encoder.layer.6.output.LayerNorm.bias', 'encoder.layer.6.output.LayerNorm.weight', 'encoder.layer.6.output.dense.bias', 'encoder.layer.6.output.dense.weight', 'encoder.layer.7.attention.output.LayerNorm.bias', 'encoder.layer.7.attention.output.LayerNorm.weight', 'encoder.layer.7.attention.output.dense.bias', 'encoder.layer.7.attention.output.dense.weight', 'encoder.layer.7.attention.self.key.bias', 'encoder.layer.7.attention.self.key.weight', 'encoder.layer.7.attention.self.query.bias', 'encoder.layer.7.attention.self.query.weight', 'encoder.layer.7.attention.self.value.bias', 'encoder.layer.7.attention.self.value.weight', 'encoder.layer.7.intermediate.dense.bias', 'encoder.layer.7.intermediate.dense.weight', 'encoder.layer.7.output.LayerNorm.bias', 'encoder.layer.7.output.LayerNorm.weight', 'encoder.layer.7.output.dense.bias', 'encoder.layer.7.output.dense.weight', 'encoder.layer.8.attention.output.LayerNorm.bias', 'encoder.layer.8.attention.output.LayerNorm.weight', 'encoder.layer.8.attention.output.dense.bias', 'encoder.layer.8.attention.output.dense.weight', 'encoder.layer.8.attention.self.key.bias', 'encoder.layer.8.attention.self.key.weight', 'encoder.layer.8.attention.self.query.bias', 'encoder.layer.8.attention.self.query.weight', 'encoder.layer.8.attention.self.value.bias', 'encoder.layer.8.attention.self.value.weight', 'encoder.layer.8.intermediate.dense.bias', 'encoder.layer.8.intermediate.dense.weight', 'encoder.layer.8.output.LayerNorm.bias', 'encoder.layer.8.output.LayerNorm.weight', 'encoder.layer.8.output.dense.bias', 'encoder.layer.8.output.dense.weight', 'encoder.layer.9.attention.output.LayerNorm.bias', 'encoder.layer.9.attention.output.LayerNorm.weight', 'encoder.layer.9.attention.output.dense.bias', 'encoder.layer.9.attention.output.dense.weight', 'encoder.layer.9.attention.self.key.bias', 'encoder.layer.9.attention.self.key.weight', 'encoder.layer.9.attention.self.query.bias', 'encoder.layer.9.attention.self.query.weight', 'encoder.layer.9.attention.self.value.bias', 'encoder.layer.9.attention.self.value.weight', 'encoder.layer.9.intermediate.dense.bias', 'encoder.layer.9.intermediate.dense.weight', 'encoder.layer.9.output.LayerNorm.bias', 'encoder.layer.9.output.LayerNorm.weight', 'encoder.layer.9.output.dense.bias', 'encoder.layer.9.output.dense.weight', 'pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "[26-11-2024 02:47:52] DEBUG: https://huggingface.co:443 \"HEAD /bert-base-uncased/resolve/main/config.json HTTP/11\" 200 0\n",
      "[26-11-2024 02:47:52] DEBUG: https://huggingface.co:443 \"HEAD /bert-base-uncased/resolve/main/config.json HTTP/11\" 200 0\n"
     ]
    }
   ],
   "source": [
    "bert_model_2 = transformers.BertModel.from_pretrained(\"bert-base-uncased\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "bert_model_2 = bert_model_2.cuda()\n",
    "\n",
    "ori_model1 = MENOTR_MODEL(args)\n",
    "ori_model2 = inference_model(bert_model_2, args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model_1 = ori_model1\n",
    "model_2 = ori_model2\n",
    "\n",
    "mentor_model = model_1.cuda()\n",
    "student_model = model_2.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[26-11-2024 02:47:53] DEBUG: Starting new HTTPS connection (1): ip-172-16-95-62.ap-south-1.compute.internal:8443\n",
      "[26-11-2024 02:47:53] ERROR: Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\n",
      "[26-11-2024 02:47:53] DEBUG: Starting new HTTPS connection (1): api.wandb.ai:443\n",
      "[26-11-2024 02:47:53] DEBUG: https://api.wandb.ai:443 \"POST /graphql HTTP/11\" 200 1990\n",
      "[26-11-2024 02:47:53] DEBUG: https://api.wandb.ai:443 \"POST /graphql HTTP/11\" 200 374\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mshubham-kumar1\u001b[0m (\u001b[33mshubham-kumar1-shl\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /home/ec2-user/.netrc\n",
      "[26-11-2024 02:47:53] DEBUG: Starting new HTTPS connection (1): api.wandb.ai:443\n",
      "[26-11-2024 02:47:54] DEBUG: https://api.wandb.ai:443 \"POST /graphql HTTP/11\" 200 1990\n",
      "[26-11-2024 02:47:54] DEBUG: https://api.wandb.ai:443 \"POST /graphql HTTP/11\" 200 374\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.18.7"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/ec2-user/SageMaker/Grammar_modelling/wandb/run-20241126_024754-69o9byg7</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/shubham-kumar1-shl/Grammar_modelling/runs/69o9byg7' target=\"_blank\">Bert_mentor_net_1e4</a></strong> to <a href='https://wandb.ai/shubham-kumar1-shl/Grammar_modelling' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/shubham-kumar1-shl/Grammar_modelling' target=\"_blank\">https://wandb.ai/shubham-kumar1-shl/Grammar_modelling</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/shubham-kumar1-shl/Grammar_modelling/runs/69o9byg7' target=\"_blank\">https://wandb.ai/shubham-kumar1-shl/Grammar_modelling/runs/69o9byg7</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages/transformers/optimization.py:591: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "0it [00:00, ?it/s]\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "inference_model.forward() missing 2 required positional arguments: 'msk_tensor' and 'seg_tensor'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[13], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m wandb\u001b[38;5;241m.\u001b[39mlogin(key\u001b[38;5;241m=\u001b[39margs\u001b[38;5;241m.\u001b[39mwandb_key)\n\u001b[1;32m      2\u001b[0m wandb\u001b[38;5;241m.\u001b[39minit(project\u001b[38;5;241m=\u001b[39margs\u001b[38;5;241m.\u001b[39mwandb_proj_name, config\u001b[38;5;241m=\u001b[39margs, name\u001b[38;5;241m=\u001b[39margs\u001b[38;5;241m.\u001b[39mwandb_run_name)\n\u001b[0;32m----> 4\u001b[0m \u001b[43mtrain_model_mentor_net\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_1\u001b[49m\u001b[43m,\u001b[49m\u001b[43mmodel_2\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrainset_reader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalidset_reader\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      6\u001b[0m wandb\u001b[38;5;241m.\u001b[39mfinish()\n",
      "Cell \u001b[0;32mIn[7], line 35\u001b[0m, in \u001b[0;36mtrain_model_mentor_net\u001b[0;34m(mentor_model, student_model, args, trainset_reader, validset_reader)\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[1;32m     34\u001b[0m     sample_features \u001b[38;5;241m=\u001b[39m loss_student\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m---> 35\u001b[0m     weights \u001b[38;5;241m=\u001b[39m \u001b[43mmentor_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43msample_features\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39msqueeze()\n\u001b[1;32m     37\u001b[0m weighted_loss_student \u001b[38;5;241m=\u001b[39m (weights \u001b[38;5;241m*\u001b[39m loss_student)\u001b[38;5;241m.\u001b[39mmean()\n\u001b[1;32m     39\u001b[0m running_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m weighted_loss_student\u001b[38;5;241m.\u001b[39mitem()\n",
      "File \u001b[0;32m~/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[0;31mTypeError\u001b[0m: inference_model.forward() missing 2 required positional arguments: 'msk_tensor' and 'seg_tensor'"
     ]
    }
   ],
   "source": [
    "wandb.login(key=args.wandb_key)\n",
    "wandb.init(project=args.wandb_proj_name, config=args, name=args.wandb_run_name)\n",
    "\n",
    "train_model_mentor_net(mentor_model,student_model, args, trainset_reader, validset_reader)\n",
    "\n",
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_pytorch_p310",
   "language": "python",
   "name": "conda_pytorch_p310"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
