{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torch in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (2.2.2)\n",
      "Requirement already satisfied: filelock in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from torch) (3.16.1)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from torch) (4.12.2)\n",
      "Requirement already satisfied: sympy in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from torch) (1.13.3)\n",
      "Requirement already satisfied: networkx in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from torch) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from torch) (3.1.4)\n",
      "Requirement already satisfied: fsspec in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from torch) (2024.10.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from jinja2->torch) (3.0.2)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from sympy->torch) (1.3.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: transformers in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (4.46.3)\n",
      "Requirement already satisfied: filelock in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from transformers) (3.16.1)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.23.2 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from transformers) (0.26.2)\n",
      "Requirement already satisfied: numpy>=1.17 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from transformers) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from transformers) (21.3)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from transformers) (6.0.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from transformers) (2024.11.6)\n",
      "Requirement already satisfied: requests in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from transformers) (2.32.3)\n",
      "Requirement already satisfied: tokenizers<0.21,>=0.20 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from transformers) (0.20.3)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from transformers) (0.4.5)\n",
      "Requirement already satisfied: tqdm>=4.27 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from transformers) (4.66.5)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.23.2->transformers) (2024.10.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.23.2->transformers) (4.12.2)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from packaging>=20.0->transformers) (3.2.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from requests->transformers) (3.4.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from requests->transformers) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from requests->transformers) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from requests->transformers) (2024.8.30)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: wandb in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (0.18.7)\n",
      "Requirement already satisfied: click!=8.0.0,>=7.1 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from wandb) (8.1.7)\n",
      "Requirement already satisfied: docker-pycreds>=0.4.0 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from wandb) (0.4.0)\n",
      "Requirement already satisfied: gitpython!=3.1.29,>=1.0.0 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from wandb) (3.1.43)\n",
      "Requirement already satisfied: platformdirs in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from wandb) (4.3.6)\n",
      "Requirement already satisfied: protobuf!=4.21.0,!=5.28.0,<6,>=3.19.0 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from wandb) (4.25.5)\n",
      "Requirement already satisfied: psutil>=5.0.0 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from wandb) (6.0.0)\n",
      "Requirement already satisfied: pyyaml in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from wandb) (6.0.2)\n",
      "Requirement already satisfied: requests<3,>=2.0.0 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from wandb) (2.32.3)\n",
      "Requirement already satisfied: sentry-sdk>=2.0.0 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from wandb) (2.18.0)\n",
      "Requirement already satisfied: setproctitle in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from wandb) (1.3.4)\n",
      "Requirement already satisfied: setuptools in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from wandb) (75.2.0)\n",
      "Requirement already satisfied: typing-extensions<5,>=4.4 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from wandb) (4.12.2)\n",
      "Requirement already satisfied: six>=1.4.0 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from docker-pycreds>=0.4.0->wandb) (1.16.0)\n",
      "Requirement already satisfied: gitdb<5,>=4.0.1 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from gitpython!=3.1.29,>=1.0.0->wandb) (4.0.11)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from requests<3,>=2.0.0->wandb) (3.4.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from requests<3,>=2.0.0->wandb) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from requests<3,>=2.0.0->wandb) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from requests<3,>=2.0.0->wandb) (2024.8.30)\n",
      "Requirement already satisfied: smmap<6,>=3.0.1 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from gitdb<5,>=4.0.1->gitpython!=3.1.29,>=1.0.0->wandb) (5.0.1)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install torch\n",
    "%pip install transformers\n",
    "%pip install wandb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cache Space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: LC_ALL=C.UTF-8\n",
      "env: LANG=C.UTF-8\n",
      "env: TRANSFORMERS_CACHE=/home/ec2-user/SageMaker/cache\n",
      "env: HF_DATASETS_CACHE=/home/ec2-user/SageMaker/cache\n",
      "env: CUDA_LAUNCH_BLOCKING=1\n"
     ]
    }
   ],
   "source": [
    "%env LC_ALL=C.UTF-8\n",
    "%env LANG=C.UTF-8\n",
    "%env TRANSFORMERS_CACHE=/home/ec2-user/SageMaker/cache\n",
    "%env HF_DATASETS_CACHE=/home/ec2-user/SageMaker/cache\n",
    "%env CUDA_LAUNCH_BLOCKING=1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import Lib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages/transformers/utils/hub.py:128: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import yaml\n",
    "import random, os\n",
    "import argparse\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from tqdm import tqdm\n",
    "from torch.autograd import Variable\n",
    "from tqdm import tqdm\n",
    "import transformers\n",
    "from torch.nn import NLLLoss\n",
    "import logging\n",
    "import json\n",
    "import torch.nn as nn\n",
    "import re\n",
    "from torch.autograd import Variable\n",
    "import wandb\n",
    "from pathlib import Path\n",
    "import math\n",
    "\n",
    "\n",
    "logger = logging.getLogger(__name__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def reform_label(tokens, label, tokenizer, max_seq_length):\n",
    "    new_tokens = list()\n",
    "    new_label = list()\n",
    "    for step, token in enumerate(tokens[:-1]):\n",
    "        split_token = tokenizer.tokenize(token)\n",
    "        if len(split_token) > 0:\n",
    "            new_tokens.extend(split_token)\n",
    "            \n",
    "    new_label.append(label)\n",
    "    new_tokens = new_tokens[:max_seq_length] + ['EOS']\n",
    "    \n",
    "    return new_tokens, new_label\n",
    "\n",
    "def tok2int_sent(example, tokenizer, max_seq_length):\n",
    "    src_tokens = example[0]\n",
    "    src_label = example[1]\n",
    "\n",
    "    src_tokens, src_label = reform_label(src_tokens, src_label, tokenizer, max_seq_length)\n",
    "\n",
    "    tokens = src_tokens\n",
    "    tokens = [\"[CLS]\"] + tokens\n",
    "    label = src_label\n",
    "    input_seg = [1] * len(tokens)\n",
    "\n",
    "    input_ids = tokenizer.convert_tokens_to_ids(tokens)\n",
    "    input_mask = [1] * len(input_ids)\n",
    "\n",
    "    max_len = max_seq_length * 2 + 3\n",
    "    padding = [0] * (max_len - len(input_ids))\n",
    "    input_ids += padding\n",
    "    input_mask += padding\n",
    "    input_seg += padding\n",
    "\n",
    "    return input_ids, input_mask, input_seg, label\n",
    "\n",
    "def tok2int_list(data, tokenizer, max_seq_length):\n",
    "    inps = list()\n",
    "    msks = list()\n",
    "    segs = list()\n",
    "    labs = list()\n",
    "    for examples in data:\n",
    "        input_ids, input_mask, input_seg, labels = tok2int_sent(examples, tokenizer, max_seq_length)\n",
    "        inps.append(input_ids)\n",
    "        msks.append(input_mask)\n",
    "        segs.append(input_seg)\n",
    "        labs.append(labels)\n",
    "        \n",
    "    return inps, msks, segs, labs\n",
    "\n",
    "\n",
    "class DataLoader(object):\n",
    "    ''' For data iteration '''\n",
    "\n",
    "    def __init__(self, data_path, tokenizer, args, test=False, batch_size=64):\n",
    "        self.cuda = args.cuda\n",
    "        self.batch_size = batch_size\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_len = args.max_len\n",
    "        self.data_path = data_path\n",
    "        self.test = test\n",
    "        self.src_flag = args.src_flag\n",
    "        self.hyp_flag = args.hyp_flag\n",
    "        examples = self.read_file(data_path)\n",
    "        self.examples = examples\n",
    "        self.total_num = len(examples)\n",
    "        if self.test:\n",
    "            self.total_step = np.ceil(self.total_num * 1.0 / batch_size)\n",
    "        else:\n",
    "            self.total_step = self.total_num / batch_size\n",
    "            self.shuffle()\n",
    "        self.step = 0\n",
    "    \n",
    "    def read_file(self, data_path):\n",
    "        data_list = list()\n",
    "        data = pd.read_csv(data_path)\n",
    "        data = data[data['grammarScore']>0]\n",
    "        data.dropna(inplace=True)\n",
    "\n",
    "        for _ , row in data.iterrows():\n",
    "            example = list()\n",
    "            line = row['transcript']\n",
    "            src_token = line.split()\n",
    "            src_label = float(row['grammarScore'])\n",
    "            example.append(src_token)\n",
    "            example.append(src_label)\n",
    "            data_list.append(example)\n",
    "        return data_list\n",
    "\n",
    "\n",
    "    def shuffle(self):\n",
    "        np.random.shuffle(self.examples)\n",
    "\n",
    "    def __iter__(self):\n",
    "        return self\n",
    "\n",
    "    def __next__(self):\n",
    "        return self.next()\n",
    "\n",
    "    def __len__(self):\n",
    "        return self._n_batch\n",
    "\n",
    "    def next(self):\n",
    "        ''' Get the next batch '''\n",
    "        if self.step < self.total_step:\n",
    "            examples = self.examples[self.step * self.batch_size : (self.step+1)*self.batch_size]\n",
    "            \n",
    "            inp, msk, seg, score = tok2int_list(examples, self.tokenizer, self.max_len)\n",
    "\n",
    "            inp_tensor = Variable(\n",
    "                torch.LongTensor(inp))\n",
    "            msk_tensor = Variable(\n",
    "                torch.LongTensor(msk))\n",
    "            seg_tensor = Variable(\n",
    "                torch.LongTensor(seg))\n",
    "            score_tensor = Variable(\n",
    "                torch.LongTensor(score))\n",
    "\n",
    "            if self.cuda:\n",
    "                inp_tensor = inp_tensor.cuda()\n",
    "                msk_tensor = msk_tensor.cuda()\n",
    "                seg_tensor = seg_tensor.cuda()\n",
    "                score_tensor = score_tensor.cuda()\n",
    "\n",
    "            self.step += 1\n",
    "            return inp_tensor, msk_tensor, seg_tensor, score_tensor\n",
    "\n",
    "        else:\n",
    "            self.step = 0\n",
    "            if not self.test:\n",
    "                self.shuffle()\n",
    "            raise StopIteration()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from scipy.stats import pearsonr,spearmanr\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import numpy as np\n",
    "\n",
    "def eval_result(predicts, labels):\n",
    "    spearman_corr, _ = spearmanr(predicts, labels)\n",
    "    corr, _ = pearsonr(predicts, labels)\n",
    "    res = {\"Prearson Corr\":corr,\"Spearman Corr\":spearman_corr,\"Eval Loss\":np.sqrt(mean_squared_error(predicts, labels))}\n",
    "    return res\n",
    "\n",
    "def eval_model(model, validset_reader):\n",
    "    model.eval()\n",
    "    predicts = list()\n",
    "    labels = list()\n",
    "    with torch.no_grad():\n",
    "        for step, (inp_tensor, msk_tensor, seg_tensor, score_tensor) in tqdm(enumerate(validset_reader)):\n",
    "            \n",
    "            score_tensor = score_tensor.to(torch.float)\n",
    "            prob = model(inp_tensor, msk_tensor, seg_tensor)\n",
    "            \n",
    "            predict = prob.type_as(score_tensor).view(-1).tolist()\n",
    "            score = score_tensor.view(-1).tolist()\n",
    "            predicts.extend(predict)\n",
    "            labels.extend(score)\n",
    "            \n",
    "        results = eval_result(predicts, labels)\n",
    "            \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def listnet_loss(y_i, z_i):\n",
    "    P_y_i = F.softmax(y_i.float(), dim=0)\n",
    "    P_z_i = F.softmax(z_i.float(), dim=0)\n",
    "    return - torch.sum(P_y_i * torch.log(P_z_i))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def train_model(model, args, trainset_reader, validset_reader):\n",
    "    saved_checkpoints = []\n",
    "    save_path = args.outdir\n",
    "    best_acc = 0.0\n",
    "    running_loss = 0.0\n",
    "    gamma = 0.99999\n",
    "    alpha = 0.3\n",
    "    \n",
    "    t_total = int(\n",
    "        trainset_reader.total_step / args.gradient_accumulation_steps * args.num_train_epochs)\n",
    "\n",
    "    param_optimizer = list(model.named_parameters())\n",
    "    no_decay = ['bias', 'LayerNorm.bias', 'LayerNorm.weight']\n",
    "    optimizer_grouped_parameters = [\n",
    "        {'params': [p for n, p in param_optimizer if not any(nd in n for nd in no_decay)], 'weight_decay': 0.01},\n",
    "        {'params': [p for n, p in param_optimizer if any(nd in n for nd in no_decay)], 'weight_decay': 0.0}\n",
    "    ]\n",
    "    optimizer = transformers.AdamW(optimizer_grouped_parameters, lr=args.learning_rate, eps=1e-8)\n",
    "    scheduler = transformers.get_linear_schedule_with_warmup(\n",
    "        optimizer, num_warmup_steps=0, num_training_steps=t_total\n",
    "    )\n",
    "\n",
    "    global_step = 0\n",
    "    for epoch in range(int(args.num_train_epochs)):\n",
    "        optimizer.zero_grad()\n",
    "        for inp_tensor, msk_tensor, seg_tensor, score_tensor in tqdm(trainset_reader):\n",
    "            model.train()\n",
    "\n",
    "            score = model(inp_tensor, msk_tensor, seg_tensor)\n",
    "            pred_score = score.view(-1)\n",
    "            score_tensor = score_tensor.view(-1)\n",
    "\n",
    "            loss = F.mse_loss(pred_score, score_tensor.to(torch.float))\n",
    "            \n",
    "            ## Hybrid Loss fixed weight\n",
    "            loss = (1-alpha)*loss + alpha*listnet_loss(score_tensor,pred_score)\n",
    "\n",
    "            ## Hybrid Loss Dynamic weight\n",
    "            \n",
    "            # lambda_ = 1/(1+math.exp(gamma*((int(args.num_train_epochs)/2)-epoch)))\n",
    "            # loss = lambda_*loss + (1-lambda_)*listnet_loss(score_tensor,pred_score)\n",
    "            \n",
    "            running_loss += loss.item()\n",
    "            if args.gradient_accumulation_steps != 0:\n",
    "                loss = loss / args.gradient_accumulation_steps\n",
    "                \n",
    "            loss.backward()\n",
    "            global_step += 1\n",
    "            \n",
    "            if global_step % args.gradient_accumulation_steps == 0:\n",
    "                optimizer.step()\n",
    "                scheduler.step()\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "        logger.info('Epoch: {}, Loss: {} ,LR : {}'.format(epoch, (running_loss / global_step),scheduler.get_last_lr()[0]))\n",
    "        \n",
    "        train_res = {\n",
    "            \"Train Loss\":(running_loss / global_step),\n",
    "            \"Learning Rate\" : scheduler.get_last_lr()[0],\n",
    "        }\n",
    "                \n",
    "        logger.info('Start eval!')\n",
    "        result_dict = eval_model(model, validset_reader)\n",
    "        logger.info(result_dict)\n",
    "        \n",
    "        train_res.update(result_dict)\n",
    "        \n",
    "        wandb.log(train_res)\n",
    "        \n",
    "        check_point_path = save_path + f\"/model_{epoch}_best.pt\"\n",
    "\n",
    "        torch.save({'epoch': epoch,\n",
    "                    'model': model.state_dict()},check_point_path)\n",
    "        \n",
    "        saved_checkpoints.append(check_point_path)\n",
    "\n",
    "        if len(saved_checkpoints) > args.max_model_save:\n",
    "            old_checkpoint = saved_checkpoints.pop(0)\n",
    "            if os.path.exists(old_checkpoint):\n",
    "                os.remove(old_checkpoint)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Attention Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class inference_model(nn.Module):\n",
    "    def __init__(self, bert_model, args):\n",
    "        super(inference_model, self).__init__()\n",
    "        self.bert_hidden_dim = args.bert_hidden_dim\n",
    "        self.pred_model = bert_model\n",
    "        self.model_name = args.bert_pretrain\n",
    "        self.max_len = args.max_len * 2 + 3\n",
    "        self.proj_hidden = nn.Linear(self.bert_hidden_dim, 1)\n",
    "\n",
    "\n",
    "    def forward(self, inp_tensor, msk_tensor, seg_tensor,score_flag=True):\n",
    "        inp_tensor = inp_tensor.view(-1, self.max_len)\n",
    "        msk_tensor = msk_tensor.view(-1, self.max_len)\n",
    "        seg_tensor = seg_tensor.view(-1, self.max_len)\n",
    "        \n",
    "        if \"bert\" in self.model_name.lower():\n",
    "            outputs = self.pred_model(inp_tensor, msk_tensor, seg_tensor)            \n",
    "        elif \"electra\" in self.model_name.lower():\n",
    "            outputs = self.pred_model(inp_tensor, msk_tensor)\n",
    "        else:\n",
    "            BaseException (\"Not implement!\")\n",
    "        \n",
    "        pred_score = self.proj_hidden(outputs.pooler_output)\n",
    "\n",
    "        return pred_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[23-11-2024 14:22:32] INFO: <__main__.Config object at 0x7f34b340cd60>\n",
      "[23-11-2024 14:22:32] DEBUG: Starting new HTTPS connection (1): huggingface.co:443\n",
      "[23-11-2024 14:22:33] DEBUG: https://huggingface.co:443 \"HEAD /bert-base-uncased/resolve/main/tokenizer_config.json HTTP/11\" 200 0\n",
      "[23-11-2024 14:22:33] INFO: Start training!\n",
      "[23-11-2024 14:22:33] INFO: loading training set\n",
      "[23-11-2024 14:22:34] INFO: loading validation set\n",
      "[23-11-2024 14:22:34] INFO: initializing estimator model\n"
     ]
    }
   ],
   "source": [
    "class Config:\n",
    "    def __init__(self, **entries):\n",
    "        self.__dict__.update(entries)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    config_file = 'config.yaml'\n",
    "    with open(config_file, \"r\") as ymlfile:\n",
    "        config_dict = yaml.load(ymlfile, Loader=yaml.FullLoader)\n",
    "\n",
    "    args = Config(**config_dict)\n",
    "\n",
    "    if not os.path.exists(args.outdir):\n",
    "        Path(args.outdir).mkdir(parents=True, exist_ok=True)\n",
    "        \n",
    "    handlers = [logging.FileHandler(os.path.abspath(args.outdir) + '/train_log.txt'), logging.StreamHandler()]\n",
    "    logging.basicConfig(format='[%(asctime)s] %(levelname)s: %(message)s', level=logging.DEBUG,\n",
    "                        datefmt='%d-%m-%Y %H:%M:%S', handlers=handlers)\n",
    "    logger.info(args)\n",
    "\n",
    "    tokenizer = transformers.BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "    \n",
    "    logger.info('Start training!')\n",
    "    logger.info(\"loading training set\")\n",
    "    trainset_reader = DataLoader(args.train_path, tokenizer, args, batch_size=args.train_batch_size)\n",
    "    logger.info(\"loading validation set\")\n",
    "    validset_reader = DataLoader(args.test_path, tokenizer, args, batch_size=args.valid_batch_size)\n",
    "    logger.info('initializing estimator model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[23-11-2024 14:22:35] DEBUG: https://huggingface.co:443 \"HEAD /bert-base-uncased/resolve/main/config.json HTTP/11\" 200 0\n",
      "[23-11-2024 14:22:35] DEBUG: https://huggingface.co:443 \"HEAD /bert-base-uncased/resolve/main/config.json HTTP/11\" 200 0\n"
     ]
    }
   ],
   "source": [
    "bert_model = transformers.BertModel.from_pretrained(\"bert-base-uncased\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "bert_model = bert_model.cuda()\n",
    "ori_model = inference_model(bert_model, args)\n",
    "model = ori_model\n",
    "model = model.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[23-11-2024 14:22:36] DEBUG: Starting new HTTPS connection (1): ip-172-16-95-62.ap-south-1.compute.internal:8443\n",
      "[23-11-2024 14:22:36] ERROR: Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\n",
      "[23-11-2024 14:22:36] DEBUG: Starting new HTTPS connection (1): api.wandb.ai:443\n",
      "[23-11-2024 14:22:37] DEBUG: https://api.wandb.ai:443 \"POST /graphql HTTP/11\" 200 1990\n",
      "[23-11-2024 14:22:37] DEBUG: https://api.wandb.ai:443 \"POST /graphql HTTP/11\" 200 374\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mshubham-kumar1\u001b[0m (\u001b[33mshubham-kumar1-shl\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /home/ec2-user/.netrc\n",
      "[23-11-2024 14:22:37] DEBUG: Starting new HTTPS connection (1): api.wandb.ai:443\n",
      "[23-11-2024 14:22:37] DEBUG: https://api.wandb.ai:443 \"POST /graphql HTTP/11\" 200 1990\n",
      "[23-11-2024 14:22:37] DEBUG: https://api.wandb.ai:443 \"POST /graphql HTTP/11\" 200 374\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.18.7"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/ec2-user/SageMaker/Grammar_modelling/wandb/run-20241123_142237-gl98wi8w</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/shubham-kumar1-shl/Grammar_modelling/runs/gl98wi8w' target=\"_blank\">Bert_hybrid_loss_fixed_weightage_1e4</a></strong> to <a href='https://wandb.ai/shubham-kumar1-shl/Grammar_modelling' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/shubham-kumar1-shl/Grammar_modelling' target=\"_blank\">https://wandb.ai/shubham-kumar1-shl/Grammar_modelling</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/shubham-kumar1-shl/Grammar_modelling/runs/gl98wi8w' target=\"_blank\">https://wandb.ai/shubham-kumar1-shl/Grammar_modelling/runs/gl98wi8w</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages/transformers/optimization.py:591: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "71it [01:32,  1.31s/it]"
     ]
    }
   ],
   "source": [
    "wandb.login(key=args.wandb_key)\n",
    "wandb.init(project=args.wandb_proj_name, config=args, name=args.wandb_run_name)\n",
    "\n",
    "train_model(model, args, trainset_reader, validset_reader)\n",
    "\n",
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_pytorch_p310",
   "language": "python",
   "name": "conda_pytorch_p310"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
